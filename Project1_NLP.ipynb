{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project1_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edponce/DoyleInvestigators/blob/master/Project1_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D64o3VubPHvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy\n",
        "import collections\n",
        "import urllib.request\n",
        "import urllib.parse\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv7gPdfwPSxq",
        "colab_type": "text"
      },
      "source": [
        "###$\\color{brown}{\\rm Corpus~Selection}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbl0IcL4PQGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CORPUS_URL = {\n",
        "    'The Valley of Fear': \"http://www.gutenberg.org/files/3289/3289.txt\",\n",
        "    'A Study of Scarlet': \"http://www.gutenberg.org/files/244/244.txt\",\n",
        "    'The Sign of the Four': \"http://www.gutenberg.org/files/2097/2097.txt\",\n",
        "    'The Hound of the Baskervilles': \"http://www.gutenberg.org/files/2852/2852.txt\",\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbEAIVRj7lbu",
        "colab_type": "text"
      },
      "source": [
        "###$\\color{brown}{\\rm Read~Web~Page~Content}$\n",
        "Read the corpus from web page to start processing. Use text in ASCII format (no BOMs) and remove Windows-based newlines '\\r'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AIatw6d7gUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus_from_url(url):\n",
        "    with urllib.request.urlopen(url) as fd:\n",
        "        text = fd.read()\n",
        "        try:\n",
        "            return text.decode('utf-8')\n",
        "        except UnicodeDecodeError:\n",
        "            return text.decode('iso-8859-1')\n",
        "\n",
        "\n",
        "def get_corpus_from_file(file):\n",
        "    with open(file) as fd:\n",
        "        return fd.read()\n",
        "\n",
        "\n",
        "def get_corpus(key):\n",
        "    def validate_url(url):\n",
        "        parsed_url = urllib.parse.urlparse(url)\n",
        "        return all([parsed_url.scheme, parsed_url.netloc, parsed_url.path])\n",
        "\n",
        "    # Check if a filename was provided\n",
        "    if os.path.isfile(key):\n",
        "        return get_corpus_from_file(key)\n",
        "    else:\n",
        "        if key in CORPUS_URL:\n",
        "            file = os.path.basename(CORPUS_URL[key])\n",
        "            if os.path.isfile(file):\n",
        "                return get_corpus_from_file(file)\n",
        "\n",
        "    # Check if a URL was provided\n",
        "    if validate_url(key):\n",
        "        return get_corpus_from_url(key)\n",
        "    else:\n",
        "        if key in CORPUS_URL:\n",
        "            url = CORPUS_URL[key]\n",
        "            if validate_url(url):\n",
        "                return get_corpus_from_url(url)\n",
        "\n",
        "    raise Exception(f\"corpus '{key}' not found\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q8QE-jl8q-0",
        "colab_type": "text"
      },
      "source": [
        "###$\\color{brown}{\\rm Split~into~Parts~and~Chapters}$\n",
        "CORE METHODS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFVpoNyQTRp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gutenberg_start_tag(text):\n",
        "    \"\"\"Find Gutenberg's start tag (and producer, if available).\n",
        "\n",
        "    Notes:\n",
        "        * re.match() searches at the beginning of strings, but there are\n",
        "          certain character combinations that are not considered strings,\n",
        "          and thus need to use re.search(), even if it is at the beginning\n",
        "          of line. An example are the asterisks in the Gutenberg START\n",
        "          tag.\n",
        "    \"\"\"\n",
        "    return re.search(\n",
        "        r'\\s*\\r?\\n'  # pre-whitespace\n",
        "        r'\\*{3}\\s*'  # 3 asterisks\n",
        "        r'start[^\\r\\n]+'  # tag text\n",
        "        r'\\s*\\*{3}'  # 3 asterisks\n",
        "        r'\\r?\\n\\s*'  # post-whitespace\n",
        "        r'(produced by.+\\r?\\n\\s*)?',  # producer line with post-whitespace\n",
        "        text\n",
        "    )\n",
        "\n",
        "\n",
        "def get_gutenberg_end_tag(text):\n",
        "    \"\"\"Find Gutenberg's end tag (and transcriber's notes, if available).\n",
        "\n",
        "    Notes:\n",
        "        * Duplicate/similar Gutenberg end tags.\n",
        "        * Use a newline before transcriber note to prevent matching similar\n",
        "          (but indented) notes at beginning of text.\n",
        "        * Use DOTALL flag to match transcriber's notes across multiple lines.\n",
        "          But be wary that using DOTALL prevents the use of '.+' for other\n",
        "          cases, so use '[^\\r\\n]' instead.\n",
        "    \"\"\"\n",
        "    return re.search(\n",
        "        r'('\n",
        "        r'(\\s*\\r?\\noriginal transcriber.+)?'  # transcriber notes with pre-whitespace\n",
        "        r'\\s*\\r?\\n'  # pre-whitespace\n",
        "        r'end[^\\r\\n]+'  # duplicate/similar tag text\n",
        "        r')?'\n",
        "        r'\\s+'  # pre-whitespace\n",
        "        r'\\*{3}\\s*'  # 3 asterisks\n",
        "        r\"end[^\\r\\n]+\"  # tag text\n",
        "        r'\\s*\\*{3}'  # 3 asterisks\n",
        "        r'\\r?\\n\\s*',  # post-whitespace\n",
        "        text, flags=re.DOTALL\n",
        "    )\n",
        "\n",
        "\n",
        "def get_gutenberg_part_labels(text):\n",
        "    \"\"\"\n",
        "    Notes:\n",
        "        * We consider the start of the text when the first part/chapter starts.\n",
        "    \"\"\"\n",
        "    return list(re.finditer(\n",
        "        r'\\s*\\r?\\n'  # pre-whitespace\n",
        "        r'('\n",
        "        r'part (\\d|[ivx])+'  # label with Arabic or Roman numbering\n",
        "        r'(-+|\\.)?'  # label-title delimiter\n",
        "        r'.*'  # title\n",
        "        r')'\n",
        "        r'\\r?\\n\\s*',  # post-whitespace\n",
        "        text\n",
        "    ))\n",
        "\n",
        "\n",
        "def get_gutenberg_chapter_labels(text):\n",
        "    \"\"\"\n",
        "    Notes:\n",
        "        * Some texts have the chapter tag and title in different lines.\n",
        "    \"\"\"\n",
        "    return list(re.finditer(\n",
        "        r'\\s*'  # pre-whitespace\n",
        "        r'\\r?\\n'  # no indentation\n",
        "        r'('\n",
        "        r'chapter (\\d|[ivx])+'  # label with Arabic or Roman numbering\n",
        "        r'(-+|\\.)?'  # label-title delimiter\n",
        "        r'(\\s{2})?'  # whitespace for titles two line apart\n",
        "        r'.*'  # title\n",
        "        r')'\n",
        "        r'\\r?\\n\\s*',  # post-whitespace\n",
        "        text\n",
        "    ))\n",
        "\n",
        "\n",
        "def get_gutenberg_epilogue_label(text):\n",
        "    return re.search(\n",
        "        r'\\s*\\r?\\n'  # pre-whitespace\n",
        "        r'epilogue'  # tag text\n",
        "        r'\\r?\\n\\s*',  # post-whitespace\n",
        "        text\n",
        "    )\n",
        "\n",
        "\n",
        "def get_toc(text):\n",
        "    \"\"\"Table of contents.\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "def get_prologue(text):\n",
        "    pass\n",
        "\n",
        "\n",
        "def get_epilogue(text):\n",
        "    epilogue_label = get_gutenberg_epilogue_label(text)\n",
        "    if epilogue_label:\n",
        "        etag = get_gutenberg_end_tag(text)\n",
        "        return epilogue_label.end(), etag.start()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoh4upkIhTf_",
        "colab_type": "text"
      },
      "source": [
        "Utility methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOlmReiDeYdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_part(text, part_num, *, part_labels=None):\n",
        "    \"\"\"Get span of a selected part.\n",
        "\n",
        "    Args:\n",
        "        part_num (int): Natural number of parts [1-N]\n",
        "    \"\"\"\n",
        "    # NOTE: This can be a required parameter, but simplifies invocation of this function.\n",
        "    if part_labels is None:\n",
        "        part_labels = get_gutenberg_part_labels(text)\n",
        "    if part_num < 1 or part_num > len(part_labels):\n",
        "        raise Exception('part number out-of-range')\n",
        "\n",
        "    start = part_labels[part_num - 1].end()\n",
        "    if part_num == len(part_labels):\n",
        "        epilogue_label = get_gutenberg_epilogue_label(text)\n",
        "        end = (\n",
        "            epilogue_label.start()\n",
        "            if epilogue_label\n",
        "            else get_gutenberg_end_tag(text).start()\n",
        "        )\n",
        "    else:\n",
        "        end = part_labels[part_num].start()\n",
        "    return start, end\n",
        "\n",
        "\n",
        "def get_parts(text):\n",
        "    part_labels = get_gutenberg_part_labels(text)\n",
        "    for part_num in range(1, len(part_labels) + 1):\n",
        "        yield get_part(text, part_num, part_labels=part_labels)\n",
        "\n",
        "\n",
        "def get_chapter(text, chapter_num, part_num=None, *, chapter_labels=None, part_labels=None):\n",
        "    \"\"\"Get span of chapter.\n",
        "\n",
        "    Args:\n",
        "        chapter_num (int): Natural number of chapters [1-N]\n",
        "\n",
        "        part_num (int): Natural number of parts [1-N]\n",
        "    \"\"\"\n",
        "    # NOTE: This can be a required value. This simplifies invocation of this function.\n",
        "    if chapter_labels is None:\n",
        "        chapter_labels = get_gutenberg_chapter_labels(text)\n",
        "    if chapter_num < 1 or chapter_num > len(chapter_labels):\n",
        "        raise Exception('chapter number out-of-range')\n",
        "\n",
        "    if part_labels is None:\n",
        "        part_labels = get_gutenberg_part_labels(text)\n",
        "    if part_num is not None and (part_num < 1 or part_num > len(part_labels)):\n",
        "        raise Exception('part number out-of-range')\n",
        "\n",
        "    if part_num is not None:\n",
        "        # Filter chapters not found in selected part\n",
        "        part = get_part(text, part_num, part_labels=part_labels)\n",
        "        chapter_labels = [\n",
        "            label\n",
        "            for label in chapter_labels\n",
        "            if label.end() >= part[0] and label.end() <= part[1]\n",
        "        ]\n",
        "\n",
        "    # Last chapter\n",
        "    start = chapter_labels[chapter_num - 1].end()\n",
        "    if chapter_num == len(chapter_labels):\n",
        "        # Last chapter of last part\n",
        "        if part_num is None or part_num == len(part_labels):\n",
        "            epilogue_label = get_gutenberg_epilogue_label(text)\n",
        "            end = (\n",
        "                epilogue_label.start()\n",
        "                if epilogue_label\n",
        "                else get_gutenberg_end_tag(text).start()\n",
        "            )\n",
        "        # Last chapter of intermediate part\n",
        "        elif part_num is None:\n",
        "            end = chapter_labels[chapter_num].start()\n",
        "        else:\n",
        "            end = part[1]\n",
        "    else:\n",
        "        end = chapter_labels[chapter_num].start()\n",
        "    return start, end\n",
        "\n",
        "\n",
        "def get_chapters(text, part_num=None):\n",
        "    \"\"\"Get iterator of chapter spans.\n",
        "\n",
        "    Args:\n",
        "        part_num (int): Natural number of parts [1-N]\n",
        "    \"\"\"\n",
        "    chapter_labels = get_gutenberg_chapter_labels(text)\n",
        "    part_labels = get_gutenberg_part_labels(text)\n",
        "\n",
        "    # Text has parts\n",
        "    if part_labels:\n",
        "        if part_num is not None and (part_num < 1 or part_num > len(part_labels)):\n",
        "            raise Exception('part number out-of-range')\n",
        "\n",
        "        for part_num in (\n",
        "            range(1, len(part_labels) + 1)\n",
        "            if part_num is None\n",
        "            else range(part_num, part_num + 1)\n",
        "        ):\n",
        "            # Filter chapters not found in current part\n",
        "            part = get_part(text, part_num, part_labels=part_labels)\n",
        "            _chapter_labels = [\n",
        "                label\n",
        "                for label in chapter_labels if label.end() >= part[0] and label.end() <= part[1]\n",
        "            ]\n",
        "            for chapter_num in range(1, len(_chapter_labels) + 1):\n",
        "                yield get_chapter(text, chapter_num, part_num, chapter_labels=_chapter_labels, part_labels=part_labels)\n",
        "\n",
        "    # Text does not has parts\n",
        "    else:\n",
        "        if part_num is not None:\n",
        "            print('Warning: no parts found, so part-related parameters are ignored')\n",
        "        for chapter_num in range(1, len(chapter_labels) + 1):\n",
        "            yield get_chapter(text, chapter_num, chapter_labels=chapter_labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_A4do2w4m1u",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization\n",
        "Get paragraphs, sentences, words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUievT-74mIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#[\\d|\"|\\w](.+\\n)+\\n*(\"(.+\\n)+\\n+)* --> get paragraphs followed by paragraphs that starts with >>\"<<\n",
        "#[\\d|\"|\\w](.+\\n)+(.+:)\\n+(.+\\n?)+  --> get paragraphs that has >>:<< followed by one more paragraph\n",
        "#[\\d|\"|\\w](.+\\n)+(.+:)\\n+(.+\\n?)+|[\\d|\"|\\w](.+\\n)+\\n*(\"(.+\\n)+\\n+)* --> union of the previous two\n",
        "\n",
        "def tokenize(text, span, regex, *, use_remaining=False):\n",
        "    def _get_tokens(text):\n",
        "        return [\n",
        "            match.span()\n",
        "            for match in re.finditer(regex, text)\n",
        "        ]\n",
        "\n",
        "    # Get tokens from text\n",
        "    # Add base offset to tokens' spans\n",
        "    tokens = [\n",
        "        (tok_span[0]+span[0], tok_span[1]+span[0])\n",
        "        for tok_span in _get_tokens(text[span[0]:span[1]])\n",
        "    ]\n",
        "\n",
        "    if use_remaining:\n",
        "        if tokens:\n",
        "            # Extend last token to end of text\n",
        "            tokens[-1] = tokens[-1][0], span[1]\n",
        "        else:\n",
        "            # Consider all text as the token\n",
        "            tokens = [span]\n",
        "\n",
        "    return tokens \n",
        "\n",
        "\n",
        "def get_paragraphs(text, span):\n",
        "    return tokenize(\n",
        "        text,\n",
        "        span, \n",
        "        r'('\n",
        "        r'([^\\r\\n]+\\r?\\n)+'  # (regular text with newline)+\n",
        "        r'('\n",
        "        r'(\\r?\\n)+'  # (newline)+\n",
        "        r'[^a-zA-Z]'  # non-alpha character: quote, number, etc.\n",
        "        r')?'  # handles case of multiple newlines but still same paragraph\n",
        "        r')+',  # (full regex)+\n",
        "        use_remaining=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_sentences(text, span):\n",
        "    return tokenize(\n",
        "        text,\n",
        "        span, \n",
        "        r'.+',\n",
        "        use_remaining=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_tokens(text, span):\n",
        "    return tokenize(\n",
        "        text,\n",
        "        span, \n",
        "        r'\\w+'  # compound alphanumeric words\n",
        "        r'('\n",
        "        r\"'\\w+\"  # contractions\n",
        "        r'|(-\\w+)+'  # tokens with inlined dashes\n",
        "        r')'\n",
        "        r'|\\w+'  # single alphanumeric words\n",
        "        r'|\\$?-?\\d+(,\\d+)*(.\\d+)?',  # numbers, decimals, monetary\n",
        "    )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOEgIn2Cnbcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_entities(text, span, entities):\n",
        "    spans = {}\n",
        "    for entity in entities:\n",
        "        spans[entity] = tokenize(\n",
        "            text,\n",
        "            span,\n",
        "            fr'{entity}',  # exact search\n",
        "                           # NOTE: Doesn't handle cases where entity name is a subpart of a non-entity token)\n",
        "        )\n",
        "    return spans\n",
        "\n",
        "\n",
        "def get_vocabulary(text, span):\n",
        "    return [\n",
        "        text[tok_span[0]:tok_span[1]]\n",
        "        for tok_span in get_tokens(text, span)\n",
        "    ]\n",
        "\n",
        "\n",
        "def generate_vocabulary_frequency(text, span):\n",
        "    freq = collections.defaultdict(int)\n",
        "    for word in get_vocabulary(text, span):\n",
        "        freq[word] += 1\n",
        "    return freq"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA8jVevEiDo0",
        "colab_type": "text"
      },
      "source": [
        "###$\\color{brown}{\\rm Process~Corpus}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAqfGF9pTXsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "10e147f2-a000-41b2-f25c-03a70f4874ac"
      },
      "source": [
        "entities = {\n",
        "    'sherlock', 'holmes',\n",
        "}\n",
        "\n",
        "corpus = get_corpus('The Valley of Fear')\n",
        "# corpus = get_corpus('A Study of Scarlet')\n",
        "_corpus = corpus.lower()\n",
        "for chapter_num, chapter_span in enumerate(get_chapters(_corpus), start=1):\n",
        "    print('=' * 40)\n",
        "    print(f'Chapter {chapter_num} - {chapter_span}')\n",
        "    print('=' * 40)\n",
        "    # print(corpus[chapter_span[0]:chapter_span[1]])\n",
        "\n",
        "    for par_span in get_paragraphs(corpus, chapter_span):\n",
        "        print(par_span)\n",
        "        # print(corpus[par_span[0]:par_span[1]])\n",
        "        # print('-' * 40)\n",
        "\n",
        "        for sent_span in get_sentences(corpus, par_span):\n",
        "            print(sent_span)\n",
        "\n",
        "            for tok_span in get_tokens(corpus, sent_span):\n",
        "                print(tok_span)\n",
        "                # print(corpus[tok_span[0]:tok_span[1]])\n",
        "        break\n",
        "\n",
        "    print(find_entities(_corpus, chapter_span, entities))\n",
        "    # print(get_vocabulary(_corpus, chapter_span))\n",
        "    print(generate_vocabulary_frequency(_corpus, chapter_span))\n",
        "    break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "Chapter 1 - (801, 17931)\n",
            "========================================\n",
            "(801, 896)\n",
            "(801, 836)\n",
            "(802, 803)\n",
            "(804, 806)\n",
            "(807, 815)\n",
            "(816, 818)\n",
            "(819, 824)\n",
            "(828, 832)\n",
            "(833, 834)\n",
            "(837, 838)\n",
            "(839, 896)\n",
            "(840, 841)\n",
            "(842, 848)\n",
            "(849, 851)\n",
            "(852, 854)\n",
            "(857, 865)\n",
            "(866, 872)\n",
            "(873, 881)\n",
            "(882, 893)\n",
            "{'holmes': [(866, 872), (1035, 1041), (2676, 2682), (2811, 2817), (4849, 4855), (5113, 5119), (6302, 6308), (6481, 6487), (6861, 6867), (7330, 7336), (8706, 8712), (13261, 13267), (13891, 13897), (14623, 14629), (15157, 15163), (16076, 16082), (16340, 16346), (16673, 16679), (17010, 17016), (17545, 17551)], 'sherlock': [(857, 865), (8697, 8705)]}\n",
            "defaultdict(<class 'int'>, {'i': 63, 'am': 8, 'inclined': 1, 'to': 59, 'think': 3, 'said': 12, 'should': 4, 'do': 9, 'so': 13, 'sherlock': 2, 'holmes': 18, 'remarked': 2, 'impatiently': 1, 'believe': 1, 'that': 47, 'one': 18, 'of': 86, 'the': 167, 'most': 1, 'long-suffering': 1, 'mortals': 1, 'but': 26, \"i'll\": 2, 'admit': 1, 'was': 26, 'annoyed': 1, 'at': 17, 'sardonic': 1, 'interruption': 1, 'really': 2, 'severely': 1, 'you': 33, 'are': 18, 'a': 57, 'little': 4, 'trying': 1, 'times': 1, 'he': 68, 'too': 4, 'much': 4, 'absorbed': 1, 'with': 25, 'his': 37, 'own': 4, 'thoughts': 1, 'give': 1, 'any': 4, 'immediate': 1, 'answer': 1, 'my': 13, 'remonstrance': 1, 'leaned': 1, 'upon': 7, 'hand': 2, 'untasted': 2, 'breakfast': 2, 'before': 6, 'him': 10, 'and': 50, 'stared': 3, 'slip': 2, 'paper': 4, 'which': 37, 'had': 21, 'just': 2, 'drawn': 1, 'from': 13, 'its': 6, 'envelope': 7, 'then': 8, 'took': 2, 'itself': 3, 'held': 1, 'it': 54, 'up': 4, 'light': 2, 'very': 15, 'carefully': 1, 'studied': 1, 'both': 3, 'exterior': 1, 'flap': 1, 'is': 75, \"porlock's\": 2, 'writing': 4, 'thoughtfully': 1, 'can': 7, 'hardly': 7, 'doubt': 5, 'though': 3, 'have': 26, 'seen': 2, 'only': 5, 'twice': 3, 'greek': 1, 'e': 1, 'peculiar': 1, 'top': 1, 'flourish': 1, 'distinctive': 1, 'if': 15, 'porlock': 12, 'must': 3, 'be': 14, 'something': 3, 'first': 3, 'importance': 2, 'speaking': 2, 'himself': 4, 'rather': 3, 'than': 6, 'me': 13, 'vexation': 1, 'disappeared': 1, 'in': 52, 'interest': 1, 'words': 9, 'awakened': 1, 'who': 4, 'asked': 1, 'watson': 23, 'nom-de-plume': 1, 'mere': 1, 'identification': 1, 'mark': 1, 'behind': 2, 'lies': 2, 'shifty': 1, 'evasive': 1, 'personality': 1, 'former': 1, 'letter': 5, 'frankly': 1, 'informed': 1, 'name': 4, 'not': 16, 'defied': 1, 'ever': 1, 'trace': 1, 'among': 2, 'teeming': 1, 'millions': 1, 'this': 24, 'great': 4, 'city': 1, 'important': 2, 'for': 18, 'man': 9, 'whom': 2, 'touch': 3, 'picture': 1, 'yourself': 2, 'pilot': 1, 'fish': 1, 'shark': 1, 'jackal': 1, 'lion': 1, 'anything': 1, 'insignificant': 1, 'companionship': 1, 'what': 14, 'formidable': 2, 'sinister': 2, 'highest': 2, 'degree': 1, 'where': 2, 'comes': 2, 'within': 2, 'purview': 1, 'heard': 1, 'speak': 1, 'professor': 4, 'moriarty': 4, 'famous': 2, 'scientific': 2, 'criminal': 2, 'as': 28, 'crooks': 1, 'blushes': 1, 'murmured': 1, 'deprecating': 1, 'voice': 2, 'about': 2, 'say': 4, 'unknown': 1, 'public': 1, 'distinct': 1, 'cried': 4, 'developing': 1, 'certain': 2, 'unexpected': 1, 'vein': 2, 'pawky': 1, 'humour': 1, 'against': 3, 'learn': 1, 'guard': 1, 'myself': 3, 'calling': 1, 'uttering': 1, 'libel': 1, 'eyes': 5, 'law': 1, 'there': 16, 'lie': 3, 'glory': 1, 'wonder': 2, 'greatest': 1, 'schemer': 1, 'all': 8, 'time': 3, 'organizer': 1, 'every': 2, 'deviltry': 2, 'controlling': 1, 'brain': 2, 'underworld': 1, 'might': 1, 'made': 1, 'or': 3, 'marred': 1, 'destiny': 1, 'nations': 1, \"that's\": 3, 'aloof': 1, 'general': 2, 'suspicion': 2, 'immune': 1, 'criticism': 1, 'admirable': 1, 'management': 1, 'self-effacement': 1, 'those': 4, 'uttered': 1, 'could': 4, 'hale': 1, 'court': 1, 'emerge': 1, 'your': 9, \"year's\": 1, 'pension': 1, 'solatium': 1, 'wounded': 1, 'character': 1, 'celebrated': 1, 'author': 1, 'dynamics': 1, 'an': 10, 'asteroid': 1, 'book': 14, 'ascends': 1, 'such': 4, 'rarefied': 1, 'heights': 1, 'pure': 3, 'mathematics': 1, 'no': 14, 'press': 1, 'capable': 1, 'criticizing': 1, 'traduce': 1, 'foul-mouthed': 1, 'doctor': 1, 'slandered': 1, 'would': 16, 'respective': 1, 'roles': 1, 'genius': 2, 'spared': 1, 'by': 7, 'lesser': 1, 'men': 1, 'our': 10, 'day': 1, 'will': 6, 'surely': 5, 'come': 5, 'may': 10, 'see': 5, 'exclaimed': 1, 'devoutly': 1, 'were': 7, 'ah': 1, 'yes': 1, 'so-called': 1, 'link': 3, 'chain': 3, 'some': 7, 'way': 3, 'attachment': 1, 'quite': 6, 'sound': 1, 'between': 2, 'ourselves': 2, 'flaw': 1, 'far': 2, 'been': 6, 'able': 2, 'test': 1, 'stronger': 1, 'weakest': 1, 'exactly': 3, 'dear': 5, 'hence': 1, 'extreme': 1, 'led': 1, 'on': 7, 'rudimentary': 1, 'aspirations': 1, 'towards': 2, 'right': 2, 'encouraged': 1, 'judicious': 1, 'stimulation': 1, 'occasional': 1, 'ten-pound': 1, 'note': 3, 'sent': 2, 'devious': 1, 'methods': 1, 'has': 8, 'once': 1, 'given': 2, 'advance': 1, 'information': 2, 'value': 2, 'anticipates': 1, 'prevents': 1, 'avenges': 1, 'crime': 1, 'cannot': 1, 'we': 18, 'cipher': 10, 'find': 1, 'communication': 1, 'nature': 3, 'indicate': 2, 'again': 2, 'flattened': 1, 'out': 4, 'unused': 1, 'plate': 1, 'rose': 1, 'leaning': 2, 'over': 4, 'down': 5, 'curious': 1, 'inscription': 1, 'ran': 1, 'follows': 1, '534': 8, 'c2': 2, '13': 1, '127': 1, '36': 1, '31': 1, '4': 1, '17': 1, '21': 1, '41': 1, 'douglas': 6, '109': 1, '293': 1, '5': 1, '37': 1, 'birlstone': 7, '26': 1, '9': 1, '47': 1, '171': 1, 'make': 3, 'obviously': 1, 'attempt': 1, 'convey': 1, 'secret': 2, 'use': 4, 'message': 8, 'without': 2, 'instance': 2, 'none': 2, 'why': 6, 'because': 3, 'many': 1, 'ciphers': 1, 'read': 3, 'easily': 1, 'apocrypha': 1, 'agony': 1, 'column': 6, 'crude': 1, 'devices': 1, 'amuse': 1, 'intelligence': 3, 'fatiguing': 1, 'different': 1, 'clearly': 4, 'reference': 2, 'page': 11, 'until': 1, 'told': 2, 'powerless': 1, 'contained': 1, 'question': 2, 'indicated': 1, 'native': 1, 'shrewdness': 1, 'innate': 1, 'cunning': 1, 'delight': 1, 'friends': 1, 'prevent': 1, 'inclosing': 1, 'same': 4, 'miscarry': 1, 'undone': 2, 'go': 2, 'wrong': 1, 'harm': 2, 'second': 4, 'post': 1, 'now': 7, 'overdue': 1, 'shall': 1, 'surprised': 1, 'does': 3, 'bring': 2, 'us': 11, 'either': 1, 'further': 2, 'explanation': 2, 'more': 4, 'probable': 1, 'volume': 5, 'these': 1, 'figures': 1, 'refer': 1, \"holmes's\": 2, 'calculation': 1, 'fulfilled': 1, 'few': 2, 'minutes': 1, 'appearance': 1, 'billy': 3, 'expecting': 1, 'opened': 1, 'actually': 2, 'signed': 1, 'added': 1, 'exultant': 1, 'unfolded': 1, 'epistle': 1, 'getting': 1, 'brow': 1, 'clouded': 1, 'however': 1, 'glanced': 1, 'contents': 1, 'disappointing': 1, 'fear': 6, 'expectations': 1, 'nothing': 3, 'trust': 1, 'mr': 5, 'says': 2, 'matter': 1, 'dangerous': 1, 'suspects': 2, 'came': 1, 'unexpectedly': 1, 'after': 3, 'addressed': 1, 'intention': 1, 'sending': 2, 'key': 1, 'cover': 1, 'gone': 1, 'hard': 2, 'please': 1, 'burn': 1, 'fred': 1, 'sat': 2, 'twisting': 1, 'fingers': 2, 'frowning': 1, 'into': 3, 'fire': 2, 'last': 2, 'guilty': 1, 'conscience': 1, 'knowing': 2, 'traitor': 1, 'accusation': 1, \"other's\": 1, 'other': 4, 'being': 4, 'presume': 1, 'less': 4, 'when': 6, 'party': 1, 'talk': 1, 'know': 1, 'they': 1, 'mean': 1, 'predominant': 1, 'them': 2, 'hum': 1, 'large': 6, 'brains': 1, 'europe': 2, 'powers': 1, 'darkness': 1, 'back': 2, 'infinite': 1, 'possibilities': 1, 'anyhow': 1, 'friend': 1, 'evidently': 1, 'scared': 1, 'senses': 1, 'kindly': 1, 'compare': 1, 'done': 2, 'tells': 1, 'ill-omened': 1, 'visit': 1, 'clear': 2, 'firm': 1, 'legible': 1, 'did': 3, 'write': 1, 'simply': 1, 'drop': 1, 'feared': 1, 'inquiry': 1, 'case': 2, 'possibly': 1, 'trouble': 1, 'course': 1, 'picked': 2, 'original': 1, 'bending': 1, 'brows': 1, \"it's\": 2, 'pretty': 1, 'maddening': 1, 'here': 2, 'beyond': 1, 'human': 1, 'power': 1, 'penetrate': 1, 'pushed': 1, 'away': 1, 'lit': 1, 'unsavoury': 1, 'pipe': 1, 'companion': 1, 'deepest': 1, 'meditations': 1, 'staring': 4, 'ceiling': 1, 'perhaps': 1, 'points': 1, 'escaped': 1, 'machiavellian': 1, 'intellect': 1, 'let': 5, 'consider': 2, 'problem': 2, 'reason': 5, \"man's\": 1, 'point': 1, 'departure': 1, 'somewhat': 2, 'vague': 1, 'narrow': 1, 'focus': 1, 'mind': 1, 'seems': 1, 'impenetrable': 1, 'indications': 2, 'well': 4, 'bad': 1, 'begins': 1, 'take': 1, 'working': 1, 'hypothesis': 1, 'particular': 1, 'refers': 1, 'already': 3, 'become': 1, 'gained': 1, 'next': 2, 'sign': 1, 'chapter': 3, 'sure': 2, 'agree': 2, 'number': 6, 'immaterial': 1, 'also': 1, 'finds': 1, 'length': 2, 'intolerable': 1, 'brilliant': 1, 'scintillating': 1, 'morning': 2, 'deceived': 1, 'begin': 1, 'visualize': 1, 'printed': 2, 'double': 3, 'columns': 2, 'each': 1, 'considerable': 1, 'since': 1, 'numbered': 1, 'document': 1, 'two': 3, 'hundred': 3, 'ninety-third': 1, 'reached': 1, 'limits': 1, 'supply': 1, 'injustice': 1, 'coruscation': 1, 'yet': 1, 'another': 1, 'brain-wave': 1, 'unusual': 1, 'instead': 2, 'intended': 2, 'plans': 1, 'nipped': 1, 'send': 2, 'clue': 1, 'seem': 1, 'thought': 1, 'difficulty': 2, 'finding': 1, 'imagined': 1, 'short': 1, 'common': 3, 'certainly': 1, 'sounds': 1, 'plausible': 1, 'contracted': 1, 'field': 1, 'search': 3, 'bible': 1, 'triumphantly': 1, 'good': 4, 'enough': 2, 'even': 2, 'accepted': 1, 'compliment': 1, 'likely': 2, 'elbow': 1, \"moriarty's\": 1, 'associates': 1, 'besides': 1, 'editions': 1, 'holy': 1, 'writ': 1, 'numerous': 1, 'suppose': 1, 'copies': 1, 'pagination': 1, 'standardized': 2, 'knows': 3, 'books': 2, 'correspond': 1, 'therein': 1, 'salvation': 1, 'narrowed': 1, 'anyone': 1, 'supposed': 1, 'possess': 1, 'bradshaw': 3, 'difficulties': 1, 'vocabulary': 2, 'nervous': 2, 'terse': 1, 'limited': 1, 'selection': 1, 'lend': 1, 'messages': 1, 'eliminate': 1, 'dictionary': 1, 'inadmissible': 1, 'left': 1, 'almanac': 4, 'excellent': 1, 'mistaken': 1, 'touched': 1, 'spot': 1, 'claims': 1, \"whitaker's\": 1, 'requisite': 1, 'pages': 1, 'reserved': 1, 'earlier': 1, 'becomes': 1, 'remember': 1, 'garrulous': 1, 'end': 2, 'desk': 1, 'substantial': 1, 'block': 1, 'print': 1, 'dealing': 1, 'perceive': 2, 'trade': 1, 'resources': 1, 'british': 1, 'india': 1, 'jot': 1, 'thirteen': 2, 'mahratta': 2, 'auspicious': 1, 'beginning': 1, 'twenty-seven': 2, 'government': 2, 'least': 1, 'makes': 1, 'sense': 1, 'irrelevant': 1, 'try': 1, 'alas': 1, 'word': 1, \"pig's\": 1, 'bristles': 1, 'finished': 1, 'spoken': 1, 'jesting': 1, 'twitching': 1, 'bushy': 2, 'eyebrows': 2, 'bespoke': 1, 'disappointment': 1, 'irritation': 1, 'helpless': 1, 'unhappy': 1, 'long': 1, 'silence': 1, 'broken': 1, 'sudden': 1, 'exclamation': 1, 'dashed': 1, 'cupboard': 1, 'emerged': 1, 'yellow-covered': 1, 'pay': 1, 'price': 1, 'up-to-date': 1, 'suffer': 1, 'usual': 1, 'penalties': 1, 'seventh': 1, 'january': 1, 'properly': 1, 'laid': 1, 'new': 1, 'old': 1, 'written': 1, 'store': 1, 'promising': 1, 'gleaming': 1, 'excitement': 1, 'thin': 1, 'twitched': 1, 'counted': 1, 'danger': 2, 'ha': 2, 'capital': 1, 'put': 1, 'soon': 1, 'rich': 2, 'country': 2, 'house': 2, 'confidence': 2, 'pressing': 2, 'fruit': 1, 'green-grocer': 1, 'thing': 1, 'laurel': 1, 'wreath': 1, 'round': 1, 'strange': 1, 'scrawled': 2, 'deciphered': 1, 'sheet': 2, 'foolscap': 1, 'knee': 1, 'queer': 1, 'scrambling': 1, 'expressing': 1, 'meaning': 2, 'contrary': 1, 'remarkably': 1, 'single': 1, 'express': 1, 'expect': 1, 'get': 3, 'everything': 1, 'want': 1, 'bound': 1, 'leave': 1, 'correspondent': 1, 'purport': 1, 'perfectly': 1, 'whoever': 1, 'residing': 1, 'stated': 1, 'gentleman': 1, 'near': 1, 'confident': 1, 'result': 1, 'workmanlike': 1, 'bit': 1, 'analysis': 1, 'impersonal': 1, 'joy': 2, 'true': 1, 'artist': 1, 'better': 2, 'work': 1, 'mourned': 1, 'darkly': 1, 'fell': 1, 'below': 1, 'high': 1, 'level': 1, 'aspired': 1, 'still': 1, 'chuckling': 1, 'success': 2, 'swung': 1, 'open': 1, 'door': 1, 'inspector': 4, 'macdonald': 3, 'scotland': 1, 'yard': 1, 'ushered': 1, 'room': 1, 'early': 3, 'days': 1, \"80's\": 1, 'alec': 1, 'having': 1, 'attained': 1, 'national': 1, 'fame': 1, 'achieved': 1, 'young': 1, 'trusted': 1, 'member': 1, 'detective': 1, 'force': 1, 'distinguished': 1, 'several': 1, 'cases': 1, 'intrusted': 1, 'tall': 1, 'bony': 1, 'figure': 1, 'gave': 1, 'promise': 1, 'exceptional': 1, 'physical': 1, 'strength': 1, 'while': 1, 'cranium': 1, 'deep-set': 1, 'lustrous': 1, 'spoke': 1, 'keen': 1, 'twinkled': 1, 'silent': 1, 'precise': 1, 'dour': 1, 'aberdonian': 1, 'accent': 1, 'career': 1, 'helped': 1, 'attain': 1, 'sole': 1, 'reward': 1, 'intellectual': 1, 'affection': 1, 'respect': 1, 'scotchman': 2, 'amateur': 1, 'colleague': 1, 'profound': 1, 'showed': 1, 'frankness': 1, 'consulted': 1, 'mediocrity': 1, 'higher': 1, 'talent': 2, 'instantly': 1, 'recognizes': 1, 'profession': 1, 'enable': 1, 'humiliation': 1, 'seeking': 1, 'assistance': 1, 'stood': 1, 'alone': 1, 'gifts': 1, 'experience': 1, 'prone': 1, 'friendship': 1, 'tolerant': 1, 'big': 1, 'smiled': 1, 'sight': 1, 'bird': 1, 'mac': 1, 'wish': 1, 'luck': 1, 'worm': 1, 'means': 1, 'mischief': 1, 'afoot': 1, 'hope': 1, 'nearer': 1, 'truth': 1, \"i'm\": 1, 'thinking': 1, 'answered': 1, 'grin': 1, 'maybe': 1, 'wee': 1, 'nip': 1, 'keep': 1, 'raw': 1, 'chill': 1, \"won't\": 1, 'smoke': 1, 'thank': 1, 'pushing': 1, 'hours': 1, 'precious': 1, 'ones': 1, 'self': 1, 'stopped': 1, 'suddenly': 1, 'look': 1, 'absolute': 1, 'amazement': 1, 'table': 1, 'enigmatic': 1, 'stammered': 1, \"what's\": 2, 'witchcraft': 1, 'wonderful': 1, 'names': 2, 'dr': 1, 'occasion': 1, 'solve': 1, 'amiss': 1, 'looked': 1, 'dazed': 1, 'astonishment': 1, 'manor': 1, 'horribly': 1, 'murdered': 1, 'night': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}