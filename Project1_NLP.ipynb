{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project1_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edponce/DoyleInvestigators/blob/master/Project1_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D64o3VubPHvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy\n",
        "import collections\n",
        "import urllib.request\n",
        "import urllib.parse\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv7gPdfwPSxq",
        "colab_type": "text"
      },
      "source": [
        "###$\\color{brown}{\\rm Corpus~Selection}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbl0IcL4PQGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CORPUS_URL = {\n",
        "    'The Valley of Fear': \"http://www.gutenberg.org/files/3289/3289.txt\",\n",
        "    'A Study of Scarlet': \"http://www.gutenberg.org/files/244/244.txt\",\n",
        "    'The Sign of the Four': \"http://www.gutenberg.org/files/2097/2097.txt\",\n",
        "    'The Hound of the Baskervilles': \"http://www.gutenberg.org/files/2852/2852.txt\",\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbEAIVRj7lbu",
        "colab_type": "text"
      },
      "source": [
        "###$\\color{brown}{\\rm Read~Web~Page~Content}$\n",
        "Read the corpus from web page to start processing. Use text in ASCII format (no BOMs) and remove Windows-based newlines '\\r'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AIatw6d7gUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus_from_url(url):\n",
        "    with urllib.request.urlopen(url) as fd:\n",
        "        text = fd.read()\n",
        "        try:\n",
        "            return text.decode('utf-8')\n",
        "        except UnicodeDecodeError:\n",
        "            return text.decode('iso-8859-1')\n",
        "\n",
        "\n",
        "def get_corpus_from_file(file):\n",
        "    with open(file) as fd:\n",
        "        return fd.read()\n",
        "\n",
        "\n",
        "def get_corpus(key):\n",
        "    def validate_url(url):\n",
        "        parsed_url = urllib.parse.urlparse(url)\n",
        "        return all([parsed_url.scheme, parsed_url.netloc, parsed_url.path])\n",
        "\n",
        "    # Check if a filename was provided\n",
        "    if os.path.isfile(key):\n",
        "        return get_corpus_from_file(key)\n",
        "    else:\n",
        "        if key in CORPUS_URL:\n",
        "            file = os.path.basename(CORPUS_URL[key])\n",
        "            if os.path.isfile(file):\n",
        "                return get_corpus_from_file(file)\n",
        "\n",
        "    # Check if a URL was provided\n",
        "    if validate_url(key):\n",
        "        return get_corpus_from_url(key)\n",
        "    else:\n",
        "        if key in CORPUS_URL:\n",
        "            url = CORPUS_URL[key]\n",
        "            if validate_url(url):\n",
        "                return get_corpus_from_url(url)\n",
        "\n",
        "    raise Exception(f\"corpus '{key}' not found\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q8QE-jl8q-0",
        "colab_type": "text"
      },
      "source": [
        "###$\\color{brown}{\\rm Split~into~Parts~and~Chapters}$\n",
        "CORE METHODS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFVpoNyQTRp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gutenberg_start_tag(text):\n",
        "    \"\"\"Find Gutenberg's start tag (and producer, if available).\n",
        "\n",
        "    Notes:\n",
        "        * re.match() searches at the beginning of strings, but there are\n",
        "          certain character combinations that are not considered strings,\n",
        "          and thus need to use re.search(), even if it is at the beginning\n",
        "          of line. An example are the asterisks in the Gutenberg START\n",
        "          tag.\n",
        "    \"\"\"\n",
        "    return re.search(\n",
        "        r'\\s*\\r?\\n'  # pre-whitespace\n",
        "        r'\\*{3}\\s*'  # 3 asterisks\n",
        "        r'start[^\\r\\n]+'  # tag text\n",
        "        r'\\s*\\*{3}'  # 3 asterisks\n",
        "        r'\\r?\\n\\s*'  # post-whitespace\n",
        "        r'(produced by.+\\r?\\n\\s*)?',  # producer line with post-whitespace\n",
        "        text\n",
        "    )\n",
        "\n",
        "\n",
        "def get_gutenberg_end_tag(text):\n",
        "    \"\"\"Find Gutenberg's end tag (and transcriber's notes, if available).\n",
        "\n",
        "    Notes:\n",
        "        * Duplicate/similar Gutenberg end tags.\n",
        "        * Use a newline before transcriber note to prevent matching similar\n",
        "          (but indented) notes at beginning of text.\n",
        "        * Use DOTALL flag to match transcriber's notes across multiple lines.\n",
        "          But be wary that using DOTALL prevents the use of '.+' for other\n",
        "          cases, so use '[^\\r\\n]' instead.\n",
        "    \"\"\"\n",
        "    return re.search(\n",
        "        r'('\n",
        "        r'(\\s*\\r?\\noriginal transcriber.+)?'  # transcriber notes with pre-whitespace\n",
        "        r'\\s*\\r?\\n'  # pre-whitespace\n",
        "        r'end[^\\r\\n]+'  # duplicate/similar tag text\n",
        "        r')?'\n",
        "        r'\\s+'  # pre-whitespace\n",
        "        r'\\*{3}\\s*'  # 3 asterisks\n",
        "        r\"end[^\\r\\n]+\"  # tag text\n",
        "        r'\\s*\\*{3}'  # 3 asterisks\n",
        "        r'\\r?\\n\\s*',  # post-whitespace\n",
        "        text, flags=re.DOTALL\n",
        "    )\n",
        "\n",
        "\n",
        "def get_gutenberg_part_labels(text):\n",
        "    \"\"\"\n",
        "    Notes:\n",
        "        * We consider the start of the text when the first part/chapter starts.\n",
        "    \"\"\"\n",
        "    return list(re.finditer(\n",
        "        r'\\s*\\r?\\n'  # pre-whitespace\n",
        "        r'('\n",
        "        r'part (\\d|[ivx])+'  # label with Arabic or Roman numbering\n",
        "        r'(-+|\\.)?'  # label-title delimiter\n",
        "        r'.*'  # title\n",
        "        r')'\n",
        "        r'\\r?\\n\\s*',  # post-whitespace\n",
        "        text\n",
        "    ))\n",
        "\n",
        "\n",
        "def get_gutenberg_chapter_labels(text):\n",
        "    \"\"\"\n",
        "    Notes:\n",
        "        * Some texts have the chapter tag and title in different lines.\n",
        "    \"\"\"\n",
        "    return list(re.finditer(\n",
        "        r'\\s*'  # pre-whitespace\n",
        "        r'\\r?\\n'  # no indentation\n",
        "        r'('\n",
        "        r'chapter (\\d|[ivx])+'  # label with Arabic or Roman numbering\n",
        "        r'(-+|\\.)?'  # label-title delimiter\n",
        "        r'(\\s{2})?'  # whitespace for titles two line apart\n",
        "        r'.*'  # title\n",
        "        r')'\n",
        "        r'\\r?\\n\\s*',  # post-whitespace\n",
        "        text\n",
        "    ))\n",
        "\n",
        "\n",
        "def get_gutenberg_epilogue_label(text):\n",
        "    return re.search(\n",
        "        r'\\s*\\r?\\n'  # pre-whitespace\n",
        "        r'epilogue'  # tag text\n",
        "        r'\\r?\\n\\s*',  # post-whitespace\n",
        "        text\n",
        "    )\n",
        "\n",
        "\n",
        "def get_toc(text):\n",
        "    \"\"\"Table of contents.\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "def get_prologue(text):\n",
        "    pass\n",
        "\n",
        "\n",
        "def get_epilogue(text):\n",
        "    epilogue_label = get_gutenberg_epilogue_label(text)\n",
        "    if epilogue_label:\n",
        "        etag = get_gutenberg_end_tag(text)\n",
        "        return epilogue_label.end(), etag.start()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoh4upkIhTf_",
        "colab_type": "text"
      },
      "source": [
        "Utility methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOlmReiDeYdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_part(text, part_num, *, part_labels=None):\n",
        "    \"\"\"Get span of a selected part.\n",
        "\n",
        "    Args:\n",
        "        part_num (int): Natural number of parts [1-N]\n",
        "    \"\"\"\n",
        "    # NOTE: This can be a required parameter, but simplifies invocation of this function.\n",
        "    if part_labels is None:\n",
        "        part_labels = get_gutenberg_part_labels(text)\n",
        "    if part_num < 1 or part_num > len(part_labels):\n",
        "        raise Exception('part number out-of-range')\n",
        "\n",
        "    start = part_labels[part_num - 1].end()\n",
        "    if part_num == len(part_labels):\n",
        "        epilogue_label = get_gutenberg_epilogue_label(text)\n",
        "        end = (\n",
        "            epilogue_label.start()\n",
        "            if epilogue_label\n",
        "            else get_gutenberg_end_tag(text).start()\n",
        "        )\n",
        "    else:\n",
        "        end = part_labels[part_num].start()\n",
        "    return start, end\n",
        "\n",
        "\n",
        "def get_parts(text):\n",
        "    part_labels = get_gutenberg_part_labels(text)\n",
        "    for part_num in range(1, len(part_labels) + 1):\n",
        "        yield get_part(text, part_num, part_labels=part_labels)\n",
        "\n",
        "\n",
        "def get_chapter(text, chapter_num, part_num=None, *, chapter_labels=None, part_labels=None):\n",
        "    \"\"\"Get span of chapter.\n",
        "\n",
        "    Args:\n",
        "        chapter_num (int): Natural number of chapters [1-N]\n",
        "\n",
        "        part_num (int): Natural number of parts [1-N]\n",
        "    \"\"\"\n",
        "    # NOTE: This can be a required value. This simplifies invocation of this function.\n",
        "    if chapter_labels is None:\n",
        "        chapter_labels = get_gutenberg_chapter_labels(text)\n",
        "    if chapter_num < 1 or chapter_num > len(chapter_labels):\n",
        "        raise Exception('chapter number out-of-range')\n",
        "\n",
        "    if part_labels is None:\n",
        "        part_labels = get_gutenberg_part_labels(text)\n",
        "    if part_num is not None and (part_num < 1 or part_num > len(part_labels)):\n",
        "        raise Exception('part number out-of-range')\n",
        "\n",
        "    if part_num is not None:\n",
        "        # Filter chapters not found in selected part\n",
        "        part = get_part(text, part_num, part_labels=part_labels)\n",
        "        chapter_labels = [\n",
        "            label\n",
        "            for label in chapter_labels\n",
        "            if label.end() >= part[0] and label.end() <= part[1]\n",
        "        ]\n",
        "\n",
        "    # Last chapter\n",
        "    start = chapter_labels[chapter_num - 1].end()\n",
        "    if chapter_num == len(chapter_labels):\n",
        "        # Last chapter of last part\n",
        "        if part_num is None or part_num == len(part_labels):\n",
        "            epilogue_label = get_gutenberg_epilogue_label(text)\n",
        "            end = (\n",
        "                epilogue_label.start()\n",
        "                if epilogue_label\n",
        "                else get_gutenberg_end_tag(text).start()\n",
        "            )\n",
        "        # Last chapter of intermediate part\n",
        "        elif part_num is None:\n",
        "            end = chapter_labels[chapter_num].start()\n",
        "        else:\n",
        "            end = part[1]\n",
        "    else:\n",
        "        end = chapter_labels[chapter_num].start()\n",
        "    return start, end\n",
        "\n",
        "\n",
        "def get_chapters(text, part_num=None):\n",
        "    \"\"\"Get iterator of chapter spans.\n",
        "\n",
        "    Args:\n",
        "        part_num (int): Natural number of parts [1-N]\n",
        "    \"\"\"\n",
        "    chapter_labels = get_gutenberg_chapter_labels(text)\n",
        "    part_labels = get_gutenberg_part_labels(text)\n",
        "\n",
        "    # Text has parts\n",
        "    if part_labels:\n",
        "        if part_num is not None and (part_num < 1 or part_num > len(part_labels)):\n",
        "            raise Exception('part number out-of-range')\n",
        "\n",
        "        for part_num in (\n",
        "            range(1, len(part_labels) + 1)\n",
        "            if part_num is None\n",
        "            else range(part_num, part_num + 1)\n",
        "        ):\n",
        "            # Filter chapters not found in current part\n",
        "            part = get_part(text, part_num, part_labels=part_labels)\n",
        "            _chapter_labels = [\n",
        "                label\n",
        "                for label in chapter_labels if label.end() >= part[0] and label.end() <= part[1]\n",
        "            ]\n",
        "            for chapter_num in range(1, len(_chapter_labels) + 1):\n",
        "                yield get_chapter(text, chapter_num, part_num, chapter_labels=_chapter_labels, part_labels=part_labels)\n",
        "\n",
        "    # Text does not has parts\n",
        "    else:\n",
        "        if part_num is not None:\n",
        "            print('Warning: no parts found, so part-related parameters are ignored')\n",
        "        for chapter_num in range(1, len(chapter_labels) + 1):\n",
        "            yield get_chapter(text, chapter_num, chapter_labels=chapter_labels)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_A4do2w4m1u",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization\n",
        "Get paragraphs, sentences, words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUievT-74mIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#[\\d|\"|\\w](.+\\n)+\\n*(\"(.+\\n)+\\n+)* --> get paragraphs followed by paragraphs that starts with >>\"<<\n",
        "#[\\d|\"|\\w](.+\\n)+(.+:)\\n+(.+\\n?)+  --> get paragraphs that has >>:<< followed by one more paragraph\n",
        "#[\\d|\"|\\w](.+\\n)+(.+:)\\n+(.+\\n?)+|[\\d|\"|\\w](.+\\n)+\\n*(\"(.+\\n)+\\n+)* --> union of the previous two\n",
        "\n",
        "def tokenize(text, span, regex, *, use_remaining=False):\n",
        "    def _get_tokens(text):\n",
        "        return [\n",
        "            match.span()\n",
        "            for match in re.finditer(regex, text)\n",
        "        ]\n",
        "\n",
        "    # Get tokens from text\n",
        "    # Add base offset to tokens' spans\n",
        "    tokens = [\n",
        "        (tok_span[0]+span[0], tok_span[1]+span[0])\n",
        "        for tok_span in _get_tokens(text[span[0]:span[1]])\n",
        "    ]\n",
        "\n",
        "    if use_remaining:\n",
        "        if tokens:\n",
        "            # Extend last token to end of text\n",
        "            tokens[-1] = tokens[-1][0], span[1]\n",
        "        else:\n",
        "            # Consider all text as the token\n",
        "            tokens = [span]\n",
        "\n",
        "    return tokens \n",
        "\n",
        "\n",
        "def get_paragraphs(text, span):\n",
        "    return tokenize(\n",
        "        text,\n",
        "        span, \n",
        "        r'('\n",
        "        r'([^\\r\\n]+\\r?\\n)+'  # (regular text with newline)+\n",
        "        r'('\n",
        "        r'(\\r?\\n)+'  # (newline)+\n",
        "        r'[^a-zA-Z]'  # non-alpha character: quote, number, etc.\n",
        "        r')?'  # handles case of multiple newlines but still same paragraph\n",
        "        r')+',  # (full regex)+\n",
        "        use_remaining=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_sentences(text, span):\n",
        "    return tokenize(\n",
        "        text,\n",
        "        span, \n",
        "        r'(([^\\.\\r\\n;M!]+\\n?)+(.\")?(M[rR][sS]?\\.\\s)?(M)?)+|M[rR][sS]?\\.\\s([^\\.;M!]+(\\.\")?(M[rR][sS]?\\.\\s)?(M)?)+',\n",
        "        use_remaining=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_tokens(text, span):\n",
        "    return tokenize(\n",
        "        text,\n",
        "        span, \n",
        "        r'\\w+'  # compound alphanumeric words\n",
        "        r'('\n",
        "        r\"'\\w+\"  # contractions\n",
        "        r'|(-\\w+)+'  # tokens with inlined dashes\n",
        "        r')'\n",
        "        r'|\\w+'  # single alphanumeric words\n",
        "        r'|\\$?-?\\d+(,\\d+)*(.\\d+)?',  # numbers, decimals, monetary\n",
        "    )"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOEgIn2Cnbcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_entities(text, span, entities):\n",
        "    spans = {}\n",
        "    for entity in entities:\n",
        "        spans[entity] = tokenize(\n",
        "            text,\n",
        "            span,\n",
        "            fr'{entity}',  # exact search\n",
        "                           # NOTE: Doesn't handle cases where entity name is a subpart of a non-entity token)\n",
        "        )\n",
        "    return spans\n",
        "\n",
        "\n",
        "def get_vocabulary(text, span):\n",
        "    return [\n",
        "        text[tok_span[0]:tok_span[1]]\n",
        "        for tok_span in get_tokens(text, span)\n",
        "    ]\n",
        "\n",
        "\n",
        "def generate_vocabulary_frequency(text, span):\n",
        "    freq = collections.defaultdict(int)\n",
        "    for word in get_vocabulary(text, span):\n",
        "        freq[word] += 1\n",
        "    return freq"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA8jVevEiDo0",
        "colab_type": "text"
      },
      "source": [
        "###$\\color{brown}{\\rm Process~Corpus}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAqfGF9pTXsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "2d012df9-612b-4039-ab93-9136d76cc0e8"
      },
      "source": [
        "entities = {\n",
        "    'sherlock', 'holmes', 'i', 'doctor'\n",
        "    'perpetrator', 'murder', 'detective', 'crime', 'death'\n",
        "}\n",
        "\n",
        "# The Valley of Fear: I = Watson (outside of quotes)\n",
        "\n",
        "corpus = get_corpus('The Valley of Fear')\n",
        "# corpus = get_corpus('A Study of Scarlet')\n",
        "_corpus = corpus.lower()\n",
        "for chapter_num, chapter_span in enumerate(get_chapters(_corpus), start=1):\n",
        "    print('=' * 40)\n",
        "    print(f'Chapter {chapter_num} - {chapter_span}')\n",
        "    print('=' * 40)\n",
        "    # print(corpus[chapter_span[0]:chapter_span[1]])\n",
        "\n",
        "    for par_span in get_paragraphs(corpus, chapter_span):\n",
        "        print(par_span)\n",
        "        print(corpus[par_span[0]:par_span[1]])\n",
        "        print('-' * 40)\n",
        "\n",
        "        for sent_span in get_sentences(corpus, par_span):\n",
        "            print(sent_span, corpus[sent_span[0]:sent_span[1]])\n",
        "\n",
        "            for tok_span in get_tokens(corpus, sent_span):\n",
        "                print(tok_span, corpus[tok_span[0]:tok_span[1]])\n",
        "        break\n",
        "    #print(find_entities(_corpus, chapter_span, entities))\n",
        "    #print(get_vocabulary(_corpus, chapter_span))\n",
        "    #print(generate_vocabulary_frequency(_corpus, chapter_span))\n",
        "    break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========================================\n",
            "Chapter 1 - (801, 17931)\n",
            "========================================\n",
            "(801, 896)\n",
            "\"I am inclined to think--\" said I.\r\n",
            "\r\n",
            "\"I should do so,\" Sherlock Holmes remarked impatiently.\r\n",
            "\n",
            "----------------------------------------\n",
            "(801, 834) \"I am inclined to think--\" said I\n",
            "(802, 803) I\n",
            "(804, 806) am\n",
            "(807, 815) inclined\n",
            "(816, 818) to\n",
            "(819, 824) think\n",
            "(828, 832) said\n",
            "(833, 834) I\n",
            "(839, 896) \"I should do so,\" Sherlock Holmes remarked impatiently.\r\n",
            "\n",
            "(840, 841) I\n",
            "(842, 848) should\n",
            "(849, 851) do\n",
            "(852, 854) so\n",
            "(857, 865) Sherlock\n",
            "(866, 872) Holmes\n",
            "(873, 881) remarked\n",
            "(882, 893) impatiently\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}