{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project1_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr2wdHfSTKj4"
      },
      "source": [
        "###$\\color{brown}{\\rm Team Members}$\n",
        "## Fabian Fallas\n",
        "## Maofeng Tang\n",
        "## Chris Gropp\n",
        "## Eduardo Ponce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv7gPdfwPSxq"
      },
      "source": [
        "###$\\color{brown}{\\rm Imports}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D64o3VubPHvS"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import copy\n",
        "import pprint\n",
        "import itertools\n",
        "import collections\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "import numpy as np\n",
        "import seaborn as sns  # 0.10.1 (0.11 fails in barplot due to API change)\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-XOfdD4g5lO"
      },
      "source": [
        "###$\\color{brown}{\\rm Corpus~Selection}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbl0IcL4PQGL"
      },
      "source": [
        "CORPUS_URL = {\n",
        "    'The Valley of Fear': \"http://www.gutenberg.org/files/3289/3289.txt\",\n",
        "    'A Study in Scarlet': \"http://www.gutenberg.org/files/244/244.txt\",\n",
        "    'The Sign of the Four': \"http://www.gutenberg.org/files/2097/2097.txt\",\n",
        "    'The Hound of the Baskervilles': \"http://www.gutenberg.org/files/2852/2852.txt\",\n",
        "    # NOTE: This file is a compilation of adventures where \"The Boscombe Valley Mystery\" is Adventure 4\n",
        "    'The Boscombe Valley Mystery': 'https://www.gutenberg.org/files/1661/1661.txt',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jum0u8PPILo6"
      },
      "source": [
        "###$\\color{brown}{\\rm Characters~and~Aliases}$\n",
        "* Names and aliases are case-sensitive\n",
        "* Underscores are used to prevent token splits when creating variant forms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLF6Xu24II7K"
      },
      "source": [
        "CHARACTERS_NAMES = {\n",
        "    'The Valley of Fear': {\n",
        "        'main': [\n",
        "            ['Sherlock Holmes'],\n",
        "            ['Watson'],\n",
        "            ['John Douglas', 'Birdy Edwards', 'Mr._Douglas', 'Steve Wilson', 'Jack McMurdo'],\n",
        "            ['McGinty', 'Boss', 'Bodymaster'],\n",
        "            ['Alec MacDonald', 'inspector', 'Inspector', 'Mr._Mac', 'Mac'],\n",
        "            ['White Mason', 'police_officer'],\n",
        "            ['Mrs._Douglas'],\n",
        "            ['Cecil James Barker'],\n",
        "            ['Ettie Shafter'],\n",
        "            ['Fred Porlock'],\n",
        "            ['Professor_Moriarty', 'professor'],\n",
        "            ['Sergeant Wilson'],\n",
        "            ['Ted Baldwin', 'Teddy Baldwin', 'Mr._Baldwin'],\n",
        "            ['Captain Marvin'],\n",
        "        ],\n",
        "        'detectives': [\n",
        "            ['Sherlock Holmes'],\n",
        "            ['Watson'],\n",
        "            ['Alec MacDonald', 'inspector', 'Inspector', 'Mr._Mac', 'Mac'],\n",
        "            ['White Mason', 'police_officer'],\n",
        "        ],\n",
        "        'perpetrators': [\n",
        "            ['Professor Moriarty', 'professor'],\n",
        "            ['Ted Baldwin', 'Teddy Baldwin', 'Mr._Baldwin'],\n",
        "        ],\n",
        "        'suspects': [\n",
        "            ['Mrs. Douglas'],\n",
        "            ['Cecil James Barker'],\n",
        "        ],\n",
        "    },\n",
        "    'A Study in Scarlet': {\n",
        "        'main': [\n",
        "            ['Sherlock Holmes'],\n",
        "            ['Watson'],\n",
        "            ['John', 'John_Ferrier'],\n",
        "            ['Lucy'],\n",
        "            ['Enoch Drebber'],\n",
        "            ['Joseph Stangerson'],\n",
        "            ['Lestrade'],\n",
        "            ['Gregson'],\n",
        "            ['Jefferson Hope', 'cabman'],\n",
        "            ['Brigham Young'],\n",
        "            ['Madame_Charpentier', 'Madame', 'Mrs._Charpentier'],\n",
        "            ['Arthur_Charpentier', 'Arthur'],\n",
        "        ],\n",
        "        'detectives': [\n",
        "            ['Sherlock Holmes'],\n",
        "            ['Watson'],\n",
        "            ['Gregson'],\n",
        "            ['Lestrade'],\n",
        "        ],\n",
        "        'perpetrators': [\n",
        "            ['Jefferson Hope', 'cabman'],  # Part 1\n",
        "            # ['Enoch Drebber'],  # Part 2\n",
        "            # ['Joseph Stangerson'],  # Part 2\n",
        "        ],\n",
        "        'suspects': [\n",
        "            ['Enoch Drebber'],  # Part 1\n",
        "            ['Joseph Stangerson'],  # Part 1\n",
        "            ['Madame_Charpentier', 'Madame', 'Mrs._Charpentier'],  # Part 1\n",
        "            ['Arthur_Charpentier', 'Arthur'],  # Part 1\n",
        "        ],\n",
        "    },\n",
        "    'The Sign of the Four': {\n",
        "        'main': [\n",
        "            ['Sherlock Holmes'],\n",
        "            ['Watson'],\n",
        "            ['Mary_Morstan', 'Miss_Morstan'],\n",
        "            ['Arthur_Morstan', 'Captain_Morstan'],\n",
        "            ['Major_Sholto', 'Major', 'major'],\n",
        "            ['Thaddeus_Sholto', 'Thaddeus'],\n",
        "            ['Bartholomew_Sholto', 'Bartholomew'],\n",
        "            ['Jonathan Small'],\n",
        "            ['Tonga'],\n",
        "        ],\n",
        "        'detectives': [\n",
        "            ['Sherlock Holmes'],\n",
        "            ['Watson'],\n",
        "        ],\n",
        "        'perpetrators': [\n",
        "            ['Jonathan Small'],\n",
        "            ['Tonga'],\n",
        "        ],\n",
        "        'suspects': [\n",
        "            ['Major_Sholto', 'Major', 'major'],\n",
        "            ['Thaddeus_Sholto', 'Thaddeus'],\n",
        "        ],\n",
        "    },\n",
        "    'The Hound of the Baskervilles': {\n",
        "        'main': [\n",
        "            ['Sherlock Holmes'],\n",
        "            ['Watson'],\n",
        "            ['James Mortimer', 'doctor', 'Doctor'],\n",
        "            ['Charles_Baskerville', 'Sir_Charles'],\n",
        "            ['Henry Baskerville', 'Sir_Henry'],\n",
        "            ['Jack', 'Stapleton'],\n",
        "            ['Miss_Stapleton', 'Beryl'],\n",
        "            ['Barrymore', 'butler'],\n",
        "            ['Selden'],\n",
        "            ['Laura Lyons'],\n",
        "        ],\n",
        "        'detectives': [\n",
        "            ['Sherlock Holmes'],\n",
        "            ['Watson'],\n",
        "            ['Lestrade'],\n",
        "        ],\n",
        "        'perpetrators': [\n",
        "            ['Jack', 'Stapleton'],\n",
        "        ],\n",
        "        'suspects': [\n",
        "            ['Barrymore', 'butler'],\n",
        "            ['Laura Lyons'],\n",
        "        ],\n",
        "    },\n",
        "    'The Boscombe Valley Mystery': {\n",
        "        'main': [\n",
        "            ['Sherlock Holmes'],\n",
        "            ['Watson'],\n",
        "            ['Lestrade'],\n",
        "            ['James McCarthy'],\n",
        "            ['John_Turner', 'Mr._Turner', 'Turner'],\n",
        "            ['Alice', 'Miss_Turner'],\n",
        "        ],\n",
        "        'detectives': [\n",
        "            ['Sherlock Holmes'],\n",
        "            ['Watson'],\n",
        "            ['Lestrade'],\n",
        "        ],\n",
        "        'perpetrators': [\n",
        "            ['John_Turner', 'Mr._Turner', 'Turner'],\n",
        "        ],\n",
        "        'suspects': [\n",
        "            ['James McCarthy'],\n",
        "        ],\n",
        "    },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbEAIVRj7lbu"
      },
      "source": [
        "###$\\color{brown}{\\rm Load~Corpus}$\n",
        "Read a corpus from web page or file to start processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AIatw6d7gUZ"
      },
      "source": [
        "def get_corpus_from_url(url):\n",
        "    with urllib.request.urlopen(url) as fd:\n",
        "        text = fd.read()\n",
        "        try:\n",
        "            return text.decode('utf-8')\n",
        "        except UnicodeDecodeError:\n",
        "            return text.decode('iso-8859-1')\n",
        "\n",
        "\n",
        "def get_corpus_from_file(file):\n",
        "    with open(file) as fd:\n",
        "        return fd.read()\n",
        "\n",
        "\n",
        "def get_corpus(key):\n",
        "    def validate_url(url):\n",
        "        parsed_url = urllib.parse.urlparse(url)\n",
        "        return all([parsed_url.scheme, parsed_url.netloc, parsed_url.path])\n",
        "\n",
        "    # Check if a filename was provided\n",
        "    if os.path.isfile(key):\n",
        "        return get_corpus_from_file(key)\n",
        "    else:\n",
        "        if key in CORPUS_URL:\n",
        "            file = os.path.basename(CORPUS_URL[key])\n",
        "            if os.path.isfile(file):\n",
        "                return get_corpus_from_file(file)\n",
        "\n",
        "    # Check if a URL was provided\n",
        "    if validate_url(key):\n",
        "        return get_corpus_from_url(key)\n",
        "    else:\n",
        "        if key in CORPUS_URL:\n",
        "            url = CORPUS_URL[key]\n",
        "            if validate_url(url):\n",
        "                return get_corpus_from_url(url)\n",
        "\n",
        "    raise Exception(f\"corpus '{key}' not found\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q8QE-jl8q-0"
      },
      "source": [
        "###$\\color{brown}{\\rm Headings~Detection~(Regex)}$\n",
        "Functions to get spans of headings:\n",
        "* Gutenberg tags\n",
        "* Named headings - parts, chapters, adventures\n",
        "* Numbered headings\n",
        "* Epilogue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFVpoNyQTRp0"
      },
      "source": [
        "def get_newline_index(text):\n",
        "    \"\"\"Find the index of the first newline in the text.\n",
        "    This is used to skip/correct one newline at beginning of headings.\n",
        "    \"\"\"\n",
        "    match = re.match(r'[ \\t\\r]*\\n', text)\n",
        "    return match.end() if match else 0\n",
        "\n",
        "\n",
        "def get_gutenberg_start_heading(text, span=None):\n",
        "    \"\"\"Find Gutenberg's start tag (and producer, if available).\n",
        "\n",
        "    Notes:\n",
        "        * re.match() searches at the beginning of strings, but there are\n",
        "          certain character combinations that are not considered strings,\n",
        "          and thus need to use re.search(), even if it is at the beginning\n",
        "          of line. An example are the asterisks in the Gutenberg START\n",
        "          tag.\n",
        "    \"\"\"\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    match = re.search(\n",
        "        r'(^\\s*|(\\s*\\n){2,})'  # pre-whitespace, no indentation\n",
        "        r'\\*{3}\\s*'  # 3 asterisks\n",
        "        r'start[^\\r\\n]+'  # tag text\n",
        "        r'\\s*\\*{3}'  # 3 asterisks\n",
        "        r'(\\s*\\nproduced by.+)?'  # producer line\n",
        "        r'(\\s*\\n){2,}',  # post-whitespace\n",
        "        text[span[0]:span[1]],\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        span = match.span()\n",
        "        offs = get_newline_index(text[span[0]:span[1]])\n",
        "        return span[0] + offs, span[1]\n",
        "\n",
        "\n",
        "def get_gutenberg_end_heading(text, span=None):\n",
        "    \"\"\"Find Gutenberg's end tag (and transcriber's notes, if available).\n",
        "\n",
        "    Notes:\n",
        "        * Duplicate/similar Gutenberg end tags.\n",
        "        * Use a newline before transcriber note to prevent matching similar\n",
        "          (but indented) notes at beginning of text.\n",
        "        * Use DOTALL flag to match transcriber's notes across multiple lines.\n",
        "          But be wary that using DOTALL prevents the use of '.+' for other\n",
        "          cases, so use '[^\\r\\n]' instead.\n",
        "    \"\"\"\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    match = re.search(\n",
        "        r'('\n",
        "        r'(\\s*\\n){2,}'  # pre-whitespace, no indentation\n",
        "        r'(original transcriber.+\\s*\\n)?'  # transcriber notes\n",
        "        r'end[^\\r\\n]+'  # duplicate/similar tag text\n",
        "        r')?'\n",
        "        r'\\s*\\n'  # pre-whitespace, no indentation\n",
        "        r'\\*{3}\\s*'  # 3 asterisks\n",
        "        r\"end[^\\r\\n]+\"  # tag text\n",
        "        r'\\s*\\*{3}'  # 3 asterisks\n",
        "        r'(\\s*\\n){2,}',  # post-whitespace\n",
        "        text[span[0]:span[1]],\n",
        "        flags=re.DOTALL,\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        span = match.span()\n",
        "        offs = get_newline_index(text[span[0]:span[1]])\n",
        "        return span[0] + offs, span[1]\n",
        "\n",
        "\n",
        "def get_named_headings(text, name, span=None):\n",
        "    \"\"\"Find named headings with title.\"\"\"\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    spans = [\n",
        "        (match.start() + span[0], match.end() + span[0])\n",
        "        for match in re.finditer(\n",
        "            r'(^(\\s*)|(\\s*\\n){2,})'  # pre-whitespace, no indentation\n",
        "            r'('\n",
        "            fr'{name}[ \\t]+(\\d+|[ivxlcd]+)'  # label with Arabic/Roman number\n",
        "            r'(-+|\\.)?'  # label-title delimiter\n",
        "            r'((\\s*\\n){2})?'  # whitespace for titles two line apart\n",
        "            r'.*(\\r?\\n.*)?'  # title (muti-line support)\n",
        "            r'|'  # cases: name # \\s* label, # name/label\n",
        "            r'(\\d+|[ivxlcd]+)'  # label with Arabic or Roman numbering\n",
        "            r'(-+|\\.)?'  # label-title delimiter\n",
        "            fr'[ \\t]+.*{name}.*'  # label with name\n",
        "            r')'\n",
        "            r'(\\s*\\n){2,}',  # post-whitespace\n",
        "            text[span[0]:span[1]],\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    _spans = []\n",
        "    for _span in spans:\n",
        "        offs = get_newline_index(text[_span[0]:_span[1]])\n",
        "        _spans.append((_span[0] + offs, _span[1]))\n",
        "    return _spans\n",
        "\n",
        "\n",
        "def get_numbered_headings(text, span=None):\n",
        "    \"\"\"Find numbered headings with no title.\"\"\"\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    spans = [\n",
        "        (match.start() + span[0], match.end() + span[0])\n",
        "        for match in re.finditer(\n",
        "            r'(^\\s*|(\\s*\\n){2,})'  # pre-whitespace, no indentation\n",
        "            fr'(\\d+|[ivxlcd]+)'  # label with Arabic or Roman numbering\n",
        "            r'(-+|\\.)'  # label-title delimiter\n",
        "            r'(\\s*\\n){2,}',  # post-whitespace\n",
        "            text[span[0]:span[1]]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    _spans = []\n",
        "    for _span in spans:\n",
        "        offs = get_newline_index(text[_span[0]:_span[1]])\n",
        "        _spans.append((_span[0] + offs, _span[1]))\n",
        "    return _spans\n",
        "\n",
        "\n",
        "def get_epilogue_heading(text, span=None):\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    match = re.search(\n",
        "        r'(^\\s*|(\\s*\\n){2,})'  # pre-whitespace, no indentation\n",
        "        r'epilogue'  # tag text\n",
        "        r'(\\s*\\n){2,}',  # post-whitespace\n",
        "        text[span[0]:span[1]]\n",
        "    )\n",
        "\n",
        "    if match:\n",
        "        span = match.span()\n",
        "        offs = get_newline_index(text[span[0]:span[1]])\n",
        "        return span[0] + offs, span[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoh4upkIhTf_"
      },
      "source": [
        "###$\\color{brown}{\\rm Regions~of~Interest~(ROI)}$\n",
        "Functions to get spans of text between headings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH5HWEjvKShk"
      },
      "source": [
        "def get_headings_map(\n",
        "    text,\n",
        "    headings=['part', 'chapter', 'adventure', 'epilogue', 'numbered'],\n",
        "):\n",
        "    \"\"\"Create a list of all heading spans, guarantees at least one set\n",
        "    of bounding spans.\n",
        "\n",
        "    Args:\n",
        "        headings (str, List[str]): Heading names to search for.\n",
        "    \"\"\"\n",
        "    if not isinstance(headings, (list, tuple, set)):\n",
        "        _headings = [headings]\n",
        "    else:\n",
        "        _headings = copy.deepcopy(headings)\n",
        "\n",
        "    headings_map = {}\n",
        "    _headings_map = {}\n",
        "\n",
        "    # Always available heading, all text\n",
        "    text_heading = '_text_'\n",
        "\n",
        "    # Ensure there is always a begin \"span\"\n",
        "    start_span = get_gutenberg_start_heading(text)\n",
        "    if not start_span:\n",
        "        start_span = 0, 0\n",
        "\n",
        "    # Ensure there is always an end \"span\"\n",
        "    end_span = get_gutenberg_end_heading(text)\n",
        "    if not end_span:\n",
        "        end_span = len(text), len(text)\n",
        "    headings_map[text_heading] = [start_span, end_span]\n",
        "    if text_heading in _headings:\n",
        "        _headings.remove(text_heading)\n",
        "\n",
        "    # Optional\n",
        "    span = get_epilogue_heading(text)\n",
        "    if span:\n",
        "        heading = 'epilogue'\n",
        "        _headings_map[heading] = [span, headings_map[text_heading][1]]\n",
        "        if heading in _headings:\n",
        "            headings_map[heading] = _headings_map[heading]\n",
        "            _headings.remove(heading)\n",
        "\n",
        "    # Optional\n",
        "    spans = get_numbered_headings(text)\n",
        "    if spans:\n",
        "        heading = 'numbered'\n",
        "        _headings_map[heading] = [*spans, headings_map[text_heading][1]]\n",
        "        if heading in _headings:\n",
        "            headings_map[heading] = _headings_map[heading]\n",
        "            _headings.remove(heading)\n",
        "\n",
        "    # Optional\n",
        "    for heading in _headings:\n",
        "        spans = get_named_headings(text, heading)\n",
        "        if spans:\n",
        "            headings_map[heading] = spans\n",
        "            if 'epilogue' in _headings_map:\n",
        "                headings_map[heading].append(_headings_map['epilogue'][0])\n",
        "            else:\n",
        "                headings_map[heading].append(headings_map[text_heading][1])\n",
        "    return headings_map\n",
        "\n",
        "\n",
        "def select_rois_spans(spans, n=None):\n",
        "    if n is None:\n",
        "        _spans = [\n",
        "            (spans[i][1], spans[i + 1][0])\n",
        "            for i in range(len(spans) - 1)\n",
        "        ]\n",
        "    else:\n",
        "        _spans = [\n",
        "            (spans[i - 1][1], spans[i][0])\n",
        "            for i in ([n] if isinstance(n, int) else n)\n",
        "            if i >= 1 and i < (len(spans))\n",
        "        ]\n",
        "    return _spans\n",
        "\n",
        "\n",
        "def remove_embedded_spans(spans):\n",
        "    non_embedded_spans = copy.deepcopy(spans)\n",
        "    for i in range(len(spans)):\n",
        "        span = spans[i]\n",
        "        for j in range(i + 1, len(spans)):\n",
        "            _span = spans[j]\n",
        "            if span[0] >= _span[0] and span[1] <= _span[1]:\n",
        "                non_embedded_spans.remove(span)\n",
        "                break\n",
        "            elif span[1] > _span[1]:\n",
        "                break\n",
        "    non_embedded_spans.sort()\n",
        "    return non_embedded_spans\n",
        "\n",
        "\n",
        "def get_nonoverlapped_spans(spans, *, join=True):\n",
        "    \"\"\"Remove fully embedded spans and join overlapped spans.\"\"\"\n",
        "    non_embedded_spans = remove_embedded_spans(spans)\n",
        "    non_embedded_spans = remove_embedded_spans(non_embedded_spans[::-1])\n",
        "    if not join:\n",
        "        return non_embedded_spans\n",
        "\n",
        "    joined_spans = []\n",
        "    for span in non_embedded_spans:\n",
        "        for _span in non_embedded_spans:\n",
        "            if span != _span:\n",
        "                joined_span = None\n",
        "                if span[0] >= _span[0] and span[0] <= _span[1]:\n",
        "                    joined_span = (_span[0], span[1])\n",
        "                elif span[1] >= _span[0] and span[1] <= _span[1]:\n",
        "                    joined_span = (span[0], _span[1])\n",
        "                if joined_span:\n",
        "                    if joined_span not in joined_spans:\n",
        "                        joined_spans.append(joined_span)\n",
        "                    break\n",
        "        else:\n",
        "            joined_spans.append(span)\n",
        "\n",
        "    nonoverlap_spans = sorted(joined_spans)\n",
        "\n",
        "    # Recurse until condition is satisfied\n",
        "    if nonoverlap_spans == spans:\n",
        "        return nonoverlap_spans\n",
        "    return get_nonoverlapped_spans(nonoverlap_spans)\n",
        "\n",
        "\n",
        "def contains_span(spans, span):\n",
        "    \"\"\"Validate if a span is contained in a collection of spans.\"\"\"\n",
        "    for _span in spans:\n",
        "        if span[0] >= _span[0] and span[1] <= _span[1]:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_rois(text, name=None, *, n=None, headings_map=None):\n",
        "    \"\"\"Get span bounding a ROI.\n",
        "\n",
        "    Args:\n",
        "        name (str): ROI\n",
        "\n",
        "        n (int, Iterable[int]): Number of ROI, [1,N]\n",
        "    \"\"\"\n",
        "    if not headings_map:\n",
        "        headings_map = get_headings_map(text)\n",
        "\n",
        "    # Always available heading, all text\n",
        "    text_heading = '_text_'\n",
        "\n",
        "    rois = []\n",
        "    if not name:\n",
        "        rois = [(\n",
        "            headings_map[text_heading][0][1],\n",
        "            headings_map[text_heading][1][0],\n",
        "        )]\n",
        "    elif name in headings_map:\n",
        "        rois = select_rois_spans(headings_map[name], n)\n",
        "\n",
        "    # If necessary, skip last inner heading\n",
        "    _rois = []\n",
        "    for roi in rois:\n",
        "        value = roi[1]\n",
        "        for spans in headings_map.values():\n",
        "            for span in spans:\n",
        "                if roi[1] > span[0] and roi[1] <= span[1]:\n",
        "                    value = span[0]\n",
        "        _rois.append((roi[0], value))\n",
        "    return _rois\n",
        "\n",
        "\n",
        "\n",
        "def get_roi(text, name, span=None, *, n=None):\n",
        "    if not span:\n",
        "        spans = get_rois(text, name, n=n)\n",
        "    else:\n",
        "        spans = [\n",
        "            (_span[0] + span[0], _span[1] + span[0])\n",
        "            for _span in get_rois(text[span[0]:span[1]], name, n=n)\n",
        "        ]\n",
        "    return spans\n",
        "\n",
        "\n",
        "def get_text_from_span(text, span=None):\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "    elif isinstance(span[0], int):\n",
        "        span = [span]\n",
        "\n",
        "    roi = ''\n",
        "    for _span in span:\n",
        "        roi += text[_span[0]:_span[1]]\n",
        "    return roi\n",
        "\n",
        "\n",
        "def get_text(text, span=None, *, n=None):\n",
        "    return get_rois(text)\n",
        "\n",
        "\n",
        "def get_parts(text, span=None, *, n=None):\n",
        "    return get_roi(text, 'part', span, n=n)\n",
        "\n",
        "\n",
        "def get_chapters(text, span=None, *, n=None):\n",
        "    return get_roi(text, 'chapter', span, n=n)\n",
        "\n",
        "\n",
        "def get_adventures(text, span=None, *, n=None):\n",
        "    return get_roi(text, 'adventure', span, n=n)\n",
        "\n",
        "\n",
        "def get_numbered_sections(text, span=None, *, n=None):\n",
        "    return get_roi(text, 'numbered', span, n=n)\n",
        "\n",
        "\n",
        "def get_epilogue(text, span=None):\n",
        "    return get_roi(text, 'epilogue', span)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlvxv22kLVR8"
      },
      "source": [
        "###$\\color{brown}{\\rm Tokenization~(Regex)}$\n",
        "Regexes to get text decomposition:\n",
        "* Paragraphs\n",
        "* Sentences\n",
        "* Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lFxOxLAKtSP"
      },
      "source": [
        "def tokenize(text, span=None, regex=r'\\w', *, use_remaining=False):\n",
        "    def _get_tokens(text):\n",
        "        return [\n",
        "            match.span()\n",
        "            for match in re.finditer(regex, text)\n",
        "        ]\n",
        "\n",
        "    if not span:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    # Get tokens from text\n",
        "    # Add base offset to tokens' spans\n",
        "    tokens = [\n",
        "        (tok_span[0] + span[0], tok_span[1] + span[0])\n",
        "        for tok_span in _get_tokens(text[span[0]:span[1]])\n",
        "    ]\n",
        "\n",
        "    if use_remaining:\n",
        "        if tokens:\n",
        "            # Extend last token to end of text\n",
        "            tokens[-1] = tokens[-1][0], span[1]\n",
        "        else:\n",
        "            # Consider all text as the token\n",
        "            tokens = [span]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def select_spans(spans, n=None):\n",
        "    if n is None:\n",
        "        _spans = spans\n",
        "    else:\n",
        "        _spans = [\n",
        "            spans[i - 1]\n",
        "            for i in ([n] if isinstance(n, int) else n)\n",
        "            if i >= 1 and i <= (len(spans))\n",
        "        ]\n",
        "    return _spans\n",
        "\n",
        "\n",
        "def get_paragraphs(text, span=None, *, n=None):\n",
        "    spans = tokenize(\n",
        "        text,\n",
        "        span,\n",
        "        r'('\n",
        "        r'([^\\r\\n]+\\r?\\n)+'  # (regular text with newline)+\n",
        "        r'('\n",
        "        r'(\\r?\\n)+'  # (newline)+\n",
        "        r'[^a-zA-Z]'  # non-alpha character: quote, number, etc.\n",
        "        r')?'  # handles case of multiple newlines but still same paragraph\n",
        "        r')+',  # (full regex)+\n",
        "        use_remaining=True,\n",
        "    )\n",
        "    return select_spans(spans, n)\n",
        "\n",
        "\n",
        "def get_sentences(text, span=None, *, n=None):\n",
        "    spans = tokenize(\n",
        "        text,\n",
        "        span,\n",
        "        r'('\n",
        "        r'([^\\.\\r\\n;M!]+(\\r?\\n)?)+'\n",
        "        r'(.\")?'\n",
        "        r'(M[rR][sS]?\\.\\s)?'\n",
        "        r'M?'\n",
        "        r')+'\n",
        "        r'|'\n",
        "        r'M[rR][sS]?'\n",
        "        r'\\.\\s'\n",
        "        r'([^\\.;M!]+(.\")?(M[rR][sS]?\\.\\s)?(M)?)+',\n",
        "    )\n",
        "    return select_spans(spans, n)\n",
        "\n",
        "\n",
        "def get_tokens(text, span=None, *, n=None):\n",
        "    spans = tokenize(\n",
        "        text,\n",
        "        span,\n",
        "        r'\\w+'  # compound alphanumeric words\n",
        "        r'('\n",
        "        r\"'\\w+\"  # contractions\n",
        "        r'|(-\\w+)+'  # tokens with inlined dashes\n",
        "        r')'\n",
        "        r'|\\w+'  # single alphanumeric words\n",
        "        r'|\\$?-?\\d+(,\\d+)*(\\.\\d+)?',  # numbers, decimals, monetary\n",
        "    )\n",
        "    return select_spans(spans, n)\n",
        "\n",
        "\n",
        "def get_conversations(text, span=None, *, n=None):\n",
        "    spans = tokenize(text, span, r'\"[^\"]+\"')\n",
        "    return select_spans(spans, n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e_Pyk86Kr0o"
      },
      "source": [
        "###$\\color{brown}{\\rm Named~Entity~Recognition~(NER)}$\n",
        "Find names of characters/places."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OZh9n7L5hU8"
      },
      "source": [
        "def abbreviate_entity(entity):\n",
        "    \"\"\"Trim names to fit in plots (F. LastName).\"\"\"\n",
        "    entity_parts = entity.split()\n",
        "    if len(entity_parts) == 1:\n",
        "        abbrv = entity\n",
        "    else:\n",
        "        # F. LastName\n",
        "        abbrv = f'{entity_parts[0][0]}. {entity_parts[-1]}'\n",
        "        # FirstName L.\n",
        "        # abbrv = f'{entity_parts[0]} {entity_parts[-1][0]}.'\n",
        "    return abbrv\n",
        "\n",
        "\n",
        "def get_entity_variants(entity, *, invalids=[]):\n",
        "    \"\"\"Given a string with whitespace, return all combinations.\"\"\"\n",
        "    if '_' in entity:\n",
        "        entity_parts = [entity.replace('_', ' ')]\n",
        "    else:\n",
        "        entity_parts = entity.split()\n",
        "\n",
        "    variants = []\n",
        "    for i in range(len(entity_parts)):\n",
        "        for j in range(i+1, len(entity_parts)+1):\n",
        "            variant = ' '.join(entity_parts[i:j])\n",
        "            if variant not in invalids:\n",
        "                variants.append(variant)\n",
        "    return variants\n",
        "\n",
        "\n",
        "def search_entity(entities, text, span=None, *, invalids=[]):\n",
        "    if not isinstance(entities, (list, tuple, set)):\n",
        "        entities = [entities]\n",
        "\n",
        "    if span is None:\n",
        "        span = (0, len(text))\n",
        "\n",
        "    entities_map = collections.defaultdict(list)\n",
        "    for entity in entities:\n",
        "        entity_variants = get_entity_variants(\n",
        "            entity,\n",
        "            invalids=invalids,\n",
        "        )\n",
        "        entity_map = get_keywords_map(text, span, entity_variants)\n",
        "        if entity_map:\n",
        "            for spans in entity_map.values():\n",
        "                entities_map[entity].extend(spans)\n",
        "\n",
        "    return {entities[0]: get_nonoverlapped_spans(list(itertools.chain.from_iterable(entities_map.values())))}\n",
        "\n",
        "def get_max_frequency_from_nested_map(nested_freq_map):\n",
        "    ymax = 0\n",
        "    for _, freq_map in nested_freq_map.items():\n",
        "        ymax = max(ymax, max(freq_map.values()))\n",
        "    return ymax\n",
        "\n",
        "\n",
        "def get_max_frequency_from_nested2_map(nested2_freq_map):\n",
        "    ymax = 0\n",
        "    for _, xmap in nested2_freq_map.items():\n",
        "        ymax = max(ymax, get_max_frequency_from_nested_map(xmap))\n",
        "    return ymax\n",
        "\n",
        "\n",
        "def ner_story_with_chapters(story, span=None):\n",
        "    story_spans = {}\n",
        "    story_counts = {}\n",
        "    corpus = get_corpus(story)\n",
        "    text = corpus.lower()\n",
        "    for character_type, character_list in CHARACTERS_NAMES[story].items():\n",
        "        story_spans[character_type] = {}\n",
        "        story_counts[character_type] = {}\n",
        "        for n, chp_span in enumerate(get_chapters(text, span), start=1):\n",
        "            story_spans[character_type][n] = {}\n",
        "            story_counts[character_type][n] = {}\n",
        "            for character_names in character_list:\n",
        "                story_spans[character_type][n].update(search_entity(character_names, corpus, chp_span))\n",
        "            story_counts[character_type][n] = convert_spans_to_counts_map(story_spans[character_type][n])\n",
        "    return story_spans, story_counts\n",
        "\n",
        "\n",
        "def ner_story(story, span=None):\n",
        "    story_spans = {}\n",
        "    story_counts = {}\n",
        "    corpus = get_corpus(story)\n",
        "    for character_type, character_list in CHARACTERS_NAMES[story].items():\n",
        "        story_spans[character_type] = {}\n",
        "        story_counts[character_type] = {}\n",
        "        story_spans[character_type][0] = {}\n",
        "        story_counts[character_type][0] = {}\n",
        "        for character_names in character_list:\n",
        "            story_spans[character_type][0].update(search_entity(character_names, corpus, span))\n",
        "            story_counts[character_type][0] = convert_spans_to_counts_map(story_spans[character_type][0])\n",
        "    return story_spans, story_counts\n",
        "\n",
        "\n",
        "def ner_story_with_paragraphs(story, span=None):\n",
        "    story_spans = {}\n",
        "    story_counts = {}\n",
        "    corpus = get_corpus(story)\n",
        "    text = corpus.lower()\n",
        "    for character_type, character_list in CHARACTERS_NAMES[story].items():\n",
        "        story_spans[character_type] = {}\n",
        "        story_counts[character_type] = {}\n",
        "        for n, par_span in enumerate(get_paragraphs(text, span), start=1):\n",
        "            story_spans[character_type][n] = {}\n",
        "            story_counts[character_type][n] = {}\n",
        "            for character_names in character_list:\n",
        "                story_spans[character_type][n].update(search_entity(character_names, corpus, par_span))\n",
        "            story_counts[character_type][n] = convert_spans_to_counts_map(story_spans[character_type][n])\n",
        "    return story_spans, story_counts\n",
        "\n",
        "\n",
        "def plot_ner_counts_story_with_chapters(story, counts, *, show=False):\n",
        "    ylim = (0, get_max_frequency_from_nested2_map(counts))\n",
        "    for character_type in CHARACTERS_NAMES[story].keys():\n",
        "        ns = int(np.ceil(np.sqrt(len(counts[character_type]))))\n",
        "        fig, axes = plt.subplots(ns, ns, constrained_layout=True)\n",
        "        axes = trim_axes(axes, len(counts[character_type]))\n",
        "        for ax, (chp, freq) in zip(axes, counts[character_type].items()):\n",
        "            # Abbreviate names for plots\n",
        "            labels = []\n",
        "            data = []\n",
        "            for k, v in freq.items():\n",
        "                labels.append(abbreviate_entity(k))\n",
        "                data.append(v)\n",
        "            barplot(data, labels, xlabel=f'Chapter {chp}', ylim=ylim, ax=ax)\n",
        "        print(f'{story} - {character_type}')\n",
        "        if show:\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def plot_ner_counts_story(story, counts, *, show=False):\n",
        "    ylim = (0, get_max_frequency_from_nested2_map(counts))\n",
        "    for character_type in CHARACTERS_NAMES[story].keys():\n",
        "        freq = counts[character_type][0]\n",
        "\n",
        "        # Abbreviate names for plots\n",
        "        labels = []\n",
        "        data = []\n",
        "        for k, v in freq.items():\n",
        "            labels.append(abbreviate_entity(k))\n",
        "            data.append(v)\n",
        "\n",
        "        barplot(data, labels, xlabel='', ylim=ylim)\n",
        "        print(f'{story} - {character_type}')\n",
        "        if show:\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def plot_ner_counts_story_with_paragraphs(story, counts, *, show=False):\n",
        "    ylim = (0, get_max_frequency_from_nested2_map(counts))\n",
        "    for character_type in CHARACTERS_NAMES[story].keys():\n",
        "        ns = int(np.ceil(np.sqrt(len(counts[character_type]))))\n",
        "        fig, axes = plt.subplots(ns, ns, constrained_layout=True)\n",
        "        axes = trim_axes(axes, len(counts[character_type]))\n",
        "        for ax, (par, freq) in zip(axes, counts[character_type].items()):\n",
        "            # Abbreviate names for plots\n",
        "            labels = []\n",
        "            data = []\n",
        "            for k, v in freq.items():\n",
        "                labels.append(abbreviate_entity(k))\n",
        "                data.append(v)\n",
        "            barplot(data, labels, xlabel=f'Paragraph {par}', ylim=ylim, ax=ax)\n",
        "        print(f'{story} - {character_type}')\n",
        "        if show:\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFV62f475M4p"
      },
      "source": [
        "###$\\color{brown}{\\rm Frequency~Count}$\n",
        "For vocabulary and keywords/entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOEgIn2Cnbcb"
      },
      "source": [
        "def get_keywords_map(text, span=None, keywords=[], *, spans_map=None):\n",
        "    if spans_map is None:\n",
        "        spans_map = collections.defaultdict(list)\n",
        "\n",
        "    # NOTE: Allow cases where token is a prefix/compound of a longer token\n",
        "    for kw in keywords:\n",
        "        spans_map[kw].extend(tokenize(\n",
        "            text,\n",
        "            span,\n",
        "            fr'(?<![a-zA-Z0-9]){kw}',\n",
        "        ))\n",
        "    return spans_map\n",
        "\n",
        "\n",
        "def generate_frequency_map(spans_map, *, threshold=None):\n",
        "    freq = collections.defaultdict(int)\n",
        "    for k, v in spans_map.items():\n",
        "        if threshold is None or len(v) >= threshold:\n",
        "            freq[k] = len(v)\n",
        "    return freq\n",
        "\n",
        "\n",
        "def convert_spans_to_counts_map(spans_map):\n",
        "    return {\n",
        "        key: len(spans)\n",
        "        for key, spans in spans_map.items()\n",
        "    }\n",
        "\n",
        "\n",
        "def get_vocabulary(text, span=None, *, vocab=None):\n",
        "    return (\n",
        "        list(vocab) if vocab else []\n",
        "    ) + [\n",
        "        get_text_from_span(text, token_span)\n",
        "        for token_span in get_tokens(text, span)\n",
        "    ]\n",
        "\n",
        "\n",
        "def get_vocabulary_map(text, span=None, *, spans_map=None):\n",
        "    if spans_map is None:\n",
        "        spans_map = collections.defaultdict(list)\n",
        "\n",
        "    for token_span in get_tokens(text, span):\n",
        "        token = get_text_from_span(text, token_span)\n",
        "        spans_map[token].append(token_span)\n",
        "    return spans_map\n",
        "\n",
        "\n",
        "def get_frequent_items(data, n=10):\n",
        "    d = collections.Counter(data)\n",
        "    return dict(d.most_common(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7r26-e6QFKB"
      },
      "source": [
        "###$\\color{brown}{\\rm First~Time~Occurrences}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX2xlrjtQLkg"
      },
      "source": [
        "def get_first_occurrences(story_spans):\n",
        "    \"\"\"Get spans of first occurrences.\"\"\"\n",
        "    firstoccurrences = {}\n",
        "    for character_type, sections in story_spans.items():\n",
        "        firstoccurrences[character_type] = {}\n",
        "        for section, characters in sections.items():\n",
        "            for character in characters.keys():\n",
        "                spans = story_spans[character_type][section][character]\n",
        "                if character not in firstoccurrences[character_type] and spans:\n",
        "                    firstoccurrences[character_type][character] = spans[0]\n",
        "    return firstoccurrences\n",
        "\n",
        "\n",
        "def get_span_location_with_chapters(text, span, search_span, *, verbose=False):\n",
        "    \"\"\"Get text elements of span occurrence.\"\"\"\n",
        "    location = {'chapter': None, 'paragraph': None, 'sentence': None, 'sentence_chp': None}\n",
        "    for ichp, chp_span in enumerate(get_chapters(text, span), start=1):\n",
        "        csent = 0  # chapter based sentence\n",
        "        for ipar, par_span in enumerate(get_paragraphs(text, chp_span), start=1):\n",
        "            for isent, sent_span in enumerate(get_sentences(text, par_span), start=1):\n",
        "                csent += 1\n",
        "                if verbose:\n",
        "                    print(f'Chapter {ichp}, Paragraph {ipar}, Sentence {isent}, Sentence (Chp) {csent}')\n",
        "                    print(get_text_from_span(text, sent_span))\n",
        "                if search_span[0] >= sent_span[0] and search_span[1] <= sent_span[1]:\n",
        "                    location['chapter'] = ichp\n",
        "                    location['paragraph'] = ipar\n",
        "                    location['sentence'] = isent\n",
        "                    location['sentence_chp'] = csent\n",
        "                    return location\n",
        "\n",
        "\n",
        "def get_span_location(text, span, search_span, *, verbose=False):\n",
        "    \"\"\"Get text elements of span occurrence.\"\"\"\n",
        "    location = {'paragraph': None, 'sentence': None, 'sentence_text': None}\n",
        "    csent = 0  # text based sentence\n",
        "    for ipar, par_span in enumerate(get_paragraphs(text, span), start=1):\n",
        "        for isent, sent_span in enumerate(get_sentences(text, par_span), start=1):\n",
        "            csent += 1\n",
        "            if verbose:\n",
        "                print(f'Paragraph {ipar}, Sentence {isent}, Sentence (Text) {csent}')\n",
        "                print(get_text_from_span(text, sent_span))\n",
        "            if search_span[0] >= sent_span[0] and search_span[1] <= sent_span[1]:\n",
        "                location['paragraph'] = ipar\n",
        "                location['sentence'] = isent\n",
        "                location['sentence_text'] = csent\n",
        "                return location"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdW55Bdl9H-2"
      },
      "source": [
        "###$\\color{brown}{\\rm Neighboring~Words}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OJH1SWm9Iyo"
      },
      "source": [
        "STOPWORDS = {\n",
        "    'yours', 'do', 'might', 'although', 'all', 'he', \"'s\", 'wherein',\n",
        "    'themselves', 'used', 'anyone', 'others', 'whom', 'off', 'doing',\n",
        "    'she', 'no', 'even', 'behind', 'them', 'done', 'none', 'beside',\n",
        "    'whither', 'yet', 'but', 'perhaps', 'one', 'its', 'thereby', 'on',\n",
        "    'hereby', 'nine', 'just', 'ca', 'front', \"'d\", 'also', 'across',\n",
        "    'seems', 'towards', 'together', 'itself', 'four', 'that', 'why', 'it',\n",
        "    'so', 'three', 'down', 'keep', 'sixty', 'empty', 'above', 'whose',\n",
        "    'between', 'hence', 'two', 'be', 'however', 'nothing', 'are', 'whence',\n",
        "    're', 'beyond', 'being', 'nowhere', 'always', 'was', 'via', 'mine',\n",
        "    'ourselves', 'quite', 'when', 'only', 'should', 'amongst', 'before',\n",
        "    'from', 'any', 'most', 'how', 'same', 'if', 'latter', 'something',\n",
        "    'fifty', 'an', 'have', 'who', 'too', 'up', 'ours', 'which',\n",
        "    'beforehand', 'else', 'sometimes', 'to', 'either', 'really', 'take',\n",
        "    'such', 'go', 'again', 'into', 'a', 'per', 'anyhow', 'anything',\n",
        "    'elsewhere', 'hundred', 'afterwards', 'we', 'since', 'yourselves',\n",
        "    'both', 'top', 'formerly', \"'m\", 'her', 'alone', 'whereas', 'becoming',\n",
        "    'what', \"n't\", 'back', 'or', \"'ll\", 'never', 'everyone', 'various',\n",
        "    'then', 'over', 'against', 'twelve', 'herself', 'those', 'they',\n",
        "    'became', 'thereafter', 'forty', 'latterly', 'seeming', 'see', 'of',\n",
        "    'much', 'thereupon', 'regarding', 'get', 'give', 'by', 'indeed', \"'ve\",\n",
        "    'thus', 'part', 'can', 'still', 'is', 'nor', 'first', 'eight',\n",
        "    'nevertheless', 'another', 'along', 'i', 'former', 'someone',\n",
        "    'without', 'noone', 'whoever', 'becomes', 'about', 'through', 'unless',\n",
        "    'fifteen', 'namely', 'anywhere', 'will', 'as', 'each', 'during', 'few',\n",
        "    'become', 'their', 'hereafter', 'could', 'third', 'thru', 'somehow',\n",
        "    'in', 'bottom', 'am', 'seem', 'otherwise', 'here', 'several', 'say',\n",
        "    'would', 'our', 'for', 'due', 'move', 'somewhere', 'under', 'himself',\n",
        "    'already', 'with', 'except', 'mostly', 'amount', 'more', 'you', 'his',\n",
        "    'almost', 'every', 'upon', 'throughout', 'often', 'below', 'been',\n",
        "    'whatever', 'eleven', 'whole', 'within', 'cannot', 'five', 'him',\n",
        "    'hers', 'yourself', 'next', 'once', 'around', \"'re\", 'thence', 'using',\n",
        "    'does', 'until', 'were', 'make', 'onto', 'us', 'your', 'while',\n",
        "    'because', 'some', 'show', 'full', 'everything', 'did', 'after',\n",
        "    'call', 'now', 'the', 'meanwhile', 'many', 'whereupon', 'everywhere',\n",
        "    'me', 'name', 'not', 'seemed', 'least', 'must', 'six', 'less',\n",
        "    'serious', 'there', 'whereby', 'whether', 'own', 'and', 'whereafter',\n",
        "    'has', 'may', 'neither', 'where', 'ever', 'wherever', 'made',\n",
        "    'moreover', 'well', 'myself', 'among', 'please', 'other', 'out',\n",
        "    'this', 'therein', 'rather', 'though', 'hereupon', 'besides', 'had',\n",
        "    'at', 'twenty', 'ten', 'these', 'my', 'than', 'side', 'nobody', 'very',\n",
        "    'last', 'sometime', 'toward', 'herein', 'whenever', 'further',\n",
        "    'anyway', 'enough', 'therefore', 'put',\n",
        "}\n",
        "\n",
        "\n",
        "# Get neighbor words (even across sentences)\n",
        "def get_neighbor_words_for_character(text, spans, characters, alias_list, *, n=3, stopwords=None):\n",
        "    neighbors = {}\n",
        "    for character, cspans in characters.items():\n",
        "        neighbors[character] = {}\n",
        "        neighbors[character]['before'] = collections.defaultdict(int)\n",
        "        neighbors[character]['after'] = collections.defaultdict(int)\n",
        "        # neighbors[character]['before_spans'] = collections.defaultdict(list)\n",
        "        # neighbors[character]['after_spans'] = collections.defaultdict(list)\n",
        "\n",
        "        aliases = []\n",
        "        for clist in alias_list:\n",
        "            if character == clist[0]:\n",
        "                aliases = [alias.lower() for alias in clist]\n",
        "                break\n",
        "\n",
        "        # Combine token and character spans \n",
        "        tmp = copy.deepcopy(spans)\n",
        "        tmp.extend(cspans)\n",
        "        spans2 = get_nonoverlapped_spans(tmp, join=False)\n",
        "\n",
        "        last_idx = 0\n",
        "        for cspan in cspans:\n",
        "            try:\n",
        "                idx = spans2.index(cspan, last_idx)\n",
        "                last_idx = idx\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            # Hack to dismiss names as neighbors\n",
        "            prev_n = 0\n",
        "            _n = 1\n",
        "            while prev_n < n:\n",
        "                prev_idx = idx - _n\n",
        "                _n += 1\n",
        "                if prev_idx >= 0:\n",
        "                    token = spans2[prev_idx]\n",
        "                    word = get_text_from_span(text, token)\n",
        "\n",
        "                    valid = True\n",
        "                    for alias in aliases:\n",
        "                        if word in alias or word.replace(' ', '_') in alias:\n",
        "                            valid = False\n",
        "                            break\n",
        "\n",
        "                    if valid and stopwords:\n",
        "                        valid = word not in stopwords\n",
        "\n",
        "                    if not valid:\n",
        "                        continue\n",
        "                    \n",
        "                    prev_n += 1\n",
        "                    neighbors[character]['before'][word] += 1\n",
        "                    # neighbors[character]['before_spans'][word].append(token)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            after_n = 0\n",
        "            _n = 1\n",
        "            while after_n < n:\n",
        "                after_idx = idx + _n\n",
        "                _n += 1\n",
        "                if after_idx < len(spans2):\n",
        "                    token = spans2[after_idx]\n",
        "                    word = get_text_from_span(text, token)\n",
        "\n",
        "                    valid = True\n",
        "                    for alias in aliases:\n",
        "                        if word in alias or word.replace(' ', '_') in alias:\n",
        "                            valid = False\n",
        "                            break\n",
        "\n",
        "                    if valid and stopwords:\n",
        "                        valid = word not in stopwords\n",
        "\n",
        "                    if not valid:\n",
        "                        continue\n",
        "                    \n",
        "                    after_n += 1\n",
        "                    neighbors[character]['after'][word] += 1\n",
        "                    # neighbors[character]['after_spans'][word].append(token)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "    return neighbors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg59EqZHUi1l"
      },
      "source": [
        "###$\\color{brown}{\\rm Visualization}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5suCpM1DUh9n"
      },
      "source": [
        "def trim_axes(axes, n):\n",
        "    axes = axes.flat\n",
        "    for ax in axes[n:]:\n",
        "        ax.remove()\n",
        "    return axes[:n]\n",
        "\n",
        "\n",
        "def visualize_co_occurrence(data, keywords_rows, closeness_words_columns):\n",
        "    # Create a dataframe from the provided data\n",
        "    df = pandas.DataFrame(\n",
        "        data,\n",
        "        index=keywords_rows,\n",
        "        columns=closeness_words_columns,\n",
        "    )\n",
        "\n",
        "    # Set plot size according to the number of cols and rows\n",
        "    plt.figure(figsize=(len(keywords_rows), len(closeness_words_columns)))\n",
        "\n",
        "    # Set color of heatmap\n",
        "    # hdl = sns.heatmap(df, cmap=\"YlGnBu\", annot=True, annot_kws={'fontsize': 20}, linewidths=.5, linecolor='black', square=True)\n",
        "    hdl = sns.heatmap(df, cmap=\"YlGnBu\", annot=True, annot_kws={'fontsize': 20}, linewidths=.5, linecolor='black', cbar=False, square=True)\n",
        "\n",
        "    # Rotate text\n",
        "    loc_x, labels_x = plt.xticks()\n",
        "    loc_y, labels_y = plt.yticks()\n",
        "    hdl.set_xticklabels(labels_x, rotation=0, fontsize=20)\n",
        "    hdl.set_yticklabels(labels_y, rotation=0, fontsize=20)\n",
        "    # sns.set(font_scale=1)\n",
        "\n",
        "\n",
        "def barplot(data, labels, *, xlabel='Word', ylabel='Frequency', ylim=None, ax=None):\n",
        "    # Prepare the data to pandas\n",
        "    data_ = [labels, data]\n",
        "    data_ = np.asarray(data_).transpose()\n",
        "\n",
        "    # Create a dataframe from the provided data\n",
        "    df = pandas.DataFrame(data_, columns=[xlabel, ylabel])\n",
        "    hdl = sns.barplot(x=xlabel, y=ylabel, data=df, palette=\"Blues_d\", ax=ax)\n",
        "    plt.setp(hdl.get_xticklabels(), rotation=0)\n",
        "    # plt.setp(hdl.get_xticklabels(), rotation=15)\n",
        "    plt.rc('xtick', labelsize=16)\n",
        "    plt.rc('ytick', labelsize=16)\n",
        "    plt.rc('axes', labelsize=20)\n",
        "\n",
        "    if ylim:\n",
        "        if ax:\n",
        "            ax.set(ylim=ylim)\n",
        "        else:\n",
        "            plt.ylim(*ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g4VrZThNgJT"
      },
      "source": [
        "# Co-occurrence matrix of perpetrators\n",
        "Co-occurrence of perpetrators across all novels against Sherlock Holmes, Watson, and neighboring words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55rQOyj6NayZ"
      },
      "source": [
        "# Example of expected format of data that the method needs\n",
        "rows_example = ['Holmes', 'Watson', 'criminal', 'treasure', 'man', 'poison', 'killed']\n",
        "\n",
        "columns_example = [\"P. Moriarty\", \"T. Baldwin\", \"J. Hope\",\"J. Small\",\"Tonga\",\"J. Stapleton\",\"J. Turner\"]\n",
        "data_example = [[1,1,0,0,0,0,0],[0,0,0,0,1,0,0],[1,1,0,0,0,1,0],[0,0,0,1,1,0,0],[0,0,0,1,0,0,1],[0,0,0,0,1,0,0],[0,0,0,0,2,0,0]]\n",
        "\n",
        "# Plot\n",
        "visualize_co_occurrence(data_example,rows_example,columns_example)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2K0IrUFN4mi"
      },
      "source": [
        "Example of word count histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgkC-LT7OASA"
      },
      "source": [
        "# Example of expected format of data that the method needs\n",
        "labels_example = [\"murder\", \"guilty\", \"suspect\", \"assassin\"]\n",
        "data_example = [5,8,3,1]\n",
        "\n",
        "# Plot\n",
        "barplot(data_example,labels_example)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J26kopyFSL6C"
      },
      "source": [
        "###$\\color{brown}{\\rm Drivers}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45kpuKOWeTav"
      },
      "source": [
        "# Get text from stories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMoI5rT9chVu"
      },
      "source": [
        "corpus = get_corpus('The Valley of Fear')\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_text(corpus_l)\n",
        "text = get_text_from_span(corpus_l, spans[0])\n",
        "print(text[:100])\n",
        "print('-' * 80)\n",
        "\n",
        "corpus = get_corpus('A Study in Scarlet')\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_parts(corpus_l, n=1)  # n = Part\n",
        "text = get_text_from_span(corpus_l, spans[0])\n",
        "print(text[:100])\n",
        "print('-' * 80)\n",
        "\n",
        "corpus = get_corpus('The Sign of the Four')\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_text(corpus_l)\n",
        "text = get_text_from_span(corpus_l, spans[0])\n",
        "print(text[:100])\n",
        "print('-' * 80)\n",
        "\n",
        "corpus = get_corpus('The Hound of the Baskervilles')\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_text(corpus_l)\n",
        "text = get_text_from_span(corpus_l, spans[0])\n",
        "print(text[:100])\n",
        "print('-' * 80)\n",
        "\n",
        "corpus = get_corpus('The Boscombe Valley Mystery')\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_adventures(corpus_l, n=4)\n",
        "text = get_text_from_span(corpus_l, spans[0])\n",
        "print(text[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5_Dh2WOgDb8"
      },
      "source": [
        "# Negative words frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOF8BoVltsmr"
      },
      "source": [
        "def get_negative_words(negative_words):\n",
        "    \"\"\"\n",
        "    Get list of negative words\n",
        "    \"\"\"\n",
        "    return [negative_words[match.start():match.end()] for match in re.finditer(r'(?<=\\n)[a-zA-Z]+(?=\\n)', negative_words)]\n",
        "    # return [negative_words[match.start():match.end()] for match in re.finditer(r'(?<=\\n)[a-zA-Z]+(?=\\n)', negative_words)]\n",
        "\n",
        "\n",
        "def get_top_negative_words(corpus, negative_list, *, top=20):\n",
        "    \"\"\"\n",
        "    Get top list of negative words\n",
        "    \"\"\"\n",
        "    frequency_list = []\n",
        "    for word in negative_list:\n",
        "        result_list = [match.span() for match in re.finditer(fr'(?<![a-zA-Z0-9]){word}', corpus)]\n",
        "        frequency_list.append(len(result_list))\n",
        "    \n",
        "    # Sort results\n",
        "    temp_list = np.asarray(list(zip(negative_list,frequency_list)), dtype = [('word', np.unicode_, 16), ('frequency', int)] )\n",
        "    sorted_list = np.sort(temp_list, order='frequency', kind=\"quicksort\")\n",
        "    sorted_list = sorted_list[::-1]\n",
        "\n",
        "    return sorted_list[:top]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it1VqaovmSHz"
      },
      "source": [
        "# Get negative words\n",
        "url = \"https://raw.githubusercontent.com/edponce/DoyleInvestigators/develop/negative-words.txt\"\n",
        "negative_words = get_corpus_from_url(url)\n",
        "result_negative = get_negative_words(negative_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJbfK9lSlJSX"
      },
      "source": [
        "story = 'The Valley of Fear'\n",
        "\n",
        "# Get corpus (use all text between Gutenberg tags)\n",
        "corpus = get_corpus(story)\n",
        "corpus_l = corpus.lower()\n",
        "text = get_text_from_span(corpus_l, get_text(corpus_l)[0])\n",
        "\n",
        "# Get count\n",
        "results = get_top_negative_words(text, result_negative, top=20)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS3UWWrylSfC"
      },
      "source": [
        "story = 'A Study in Scarlet'\n",
        "\n",
        "# Get corpus (use all text between Gutenberg tags)\n",
        "corpus = get_corpus(story)\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_parts(corpus_l, n=1) # n = Part\n",
        "text = get_text_from_span(corpus_l, spans[0])\n",
        "\n",
        "# Get count\n",
        "results = get_top_negative_words(text, result_negative, top=20)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYj-DHkPl4_-"
      },
      "source": [
        "story = 'The Sign of the Four'\n",
        "\n",
        "# Get corpus (use all text between Gutenberg tags)\n",
        "corpus = get_corpus(story)\n",
        "corpus_l = corpus.lower()\n",
        "text = get_text_from_span(corpus_l, get_text(corpus_l)[0])\n",
        "\n",
        "# Get count\n",
        "results = get_top_negative_words(text, result_negative, top=20)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUIzkHwUmClo"
      },
      "source": [
        "story = 'The Hound of the Baskervilles'\n",
        "\n",
        "# Get corpus (use all text between Gutenberg tags)\n",
        "corpus = get_corpus(story)\n",
        "corpus_l = corpus.lower()\n",
        "text = get_text_from_span(corpus_l, get_text(corpus_l)[0])\n",
        "\n",
        "# Get count\n",
        "results = get_top_negative_words(text, result_negative, top=20)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1GGmcEKmHdp"
      },
      "source": [
        "story = 'The Boscombe Valley Mystery'\n",
        "\n",
        "# Get corpus (use all text between Gutenberg tags)\n",
        "corpus = get_corpus(story)\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_adventures(corpus_l, n=4)\n",
        "text = get_text_from_span(corpus_l, spans[0])\n",
        "\n",
        "# Get count\n",
        "results = get_top_negative_words(text, result_negative, top=20)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JGmhR6ePSgI"
      },
      "source": [
        "# Crime Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOHVgrENEUb2"
      },
      "source": [
        "def crime_story_with_chapters(keywords, text, span=None):\n",
        "    story_spans = {}\n",
        "    story_counts = {}\n",
        "    for n, chp_span in enumerate(get_chapters(text, span), start=1):\n",
        "        story_spans[n] = {}\n",
        "        story_counts[n] = {}\n",
        "        for kw in keywords:\n",
        "            story_spans[n].update(search_entity(kw, text, chp_span))\n",
        "        story_counts[n] = convert_spans_to_counts_map(story_spans[n])\n",
        "    return story_spans, story_counts\n",
        "\n",
        "\n",
        "def crime_story(keywords, text, span=None):\n",
        "    story_spans = {}\n",
        "    story_counts = {}\n",
        "    story_spans[0] = {}\n",
        "    story_counts[0] = {}\n",
        "    for kw in keywords:\n",
        "        story_spans[0].update(search_entity(kw, text, span))\n",
        "    story_counts[0] = convert_spans_to_counts_map(story_spans[0])\n",
        "    return story_spans, story_counts\n",
        "\n",
        "\n",
        "def get_first_crime(story_spans):\n",
        "    \"\"\"Get spans of first crime occurrences.\"\"\"\n",
        "    firstoccurrences = {}\n",
        "    for section, keywords in story_spans.items():\n",
        "        for kw, spans in keywords.items():\n",
        "            if kw not in firstoccurrences and spans:\n",
        "                firstoccurrences[kw] = spans[0]\n",
        "    return firstoccurrences\n",
        "\n",
        "\n",
        "def plot_crime_counts_story_with_chapters(story, counts, *, show=False):\n",
        "    ylim = (0, get_max_frequency_from_nested_map(counts))\n",
        "    ns = int(np.ceil(np.sqrt(len(counts))))\n",
        "    fig, axes = plt.subplots(ns, ns, constrained_layout=True)\n",
        "    axes = trim_axes(axes, len(counts))\n",
        "    for ax, (chp, freq) in zip(axes, counts.items()):\n",
        "        # Abbreviate names for plots\n",
        "        labels = []\n",
        "        data = []\n",
        "        for k, v in freq.items():\n",
        "            labels.append(k)\n",
        "            data.append(v)\n",
        "        barplot(data, labels, xlabel=f'Chapter {chp}', ylim=ylim, ax=ax)\n",
        "    print(f'{story}')\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_ner_counts_story(story, counts, *, show=False):\n",
        "    ylim = (0, get_max_frequency_from_nested2_map(counts))\n",
        "    for character_type in CHARACTERS_NAMES[story].keys():\n",
        "        freq = counts[character_type][0]\n",
        "\n",
        "        # Abbreviate names for plots\n",
        "        labels = []\n",
        "        data = []\n",
        "        for k, v in freq.items():\n",
        "            labels.append(abbreviate_entity(k))\n",
        "            data.append(v)\n",
        "\n",
        "        barplot(data, labels, xlabel='', ylim=ylim)\n",
        "        print(f'{story} - {character_type}')\n",
        "        if show:\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idDHTu4ICWg3"
      },
      "source": [
        "# Search keywords\n",
        "CRIME_KEYWORDS = [\n",
        "    'dead', 'death', 'murder', 'crime', 'hurt', 'blood', 'treasure', 'suffer',\n",
        "    'guilty', 'assassin', 'pain', 'theft', 'steal', 'victim', 'poison',\n",
        "    'gunshot', 'criminal', 'wound', 'attack',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7il19VSIVst"
      },
      "source": [
        "### The Valley of Fear, The Sign of the Four, The Hound of the Baskervilles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfllduiHIX-m"
      },
      "source": [
        "story = 'The Valley of Fear'\n",
        "#story = 'The Sign of the Four'\n",
        "#story = 'The Hound of the Baskervilles'\n",
        "\n",
        "# Get corpus span\n",
        "corpus = get_corpus(story)\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_text(corpus_l)\n",
        "span = spans[0]\n",
        "\n",
        "\n",
        "# Full story\n",
        "story_spans, story_counts = crime_story_with_chapters(CRIME_KEYWORDS, corpus_l, span)\n",
        "plot_crime_counts_story_with_chapters(story, story_counts, show=True)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "#pp.pprint(story_spans)\n",
        "pp.pprint(story_counts)\n",
        "\n",
        "\n",
        "# Get span of first occurrence\n",
        "first_occurrences = get_first_crime(story_spans)\n",
        "# print(first_occurrences)\n",
        "\n",
        "# Find location of first span\n",
        "first_locations = {}\n",
        "for kw, kw_span in first_occurrences.items():\n",
        "    location = get_span_location(corpus_l, span, kw_span, verbose=False)\n",
        "    first_locations[kw] = location\n",
        "\n",
        "print(story)\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "pp.pprint(first_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tPx_UcYJUQD"
      },
      "source": [
        "### A Study in Scarlet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5emLis8UJZ6Q"
      },
      "source": [
        "story = 'A Study in Scarlet'\n",
        "\n",
        "# Get corpus span\n",
        "corpus = get_corpus(story)\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_parts(corpus_l, n=1)  # n = Part\n",
        "span = spans[0]\n",
        "\n",
        "\n",
        "# Full story\n",
        "story_spans, story_counts = crime_story_with_chapters(CRIME_KEYWORDS, corpus_l, span)\n",
        "plot_crime_counts_story_with_chapters(story, story_counts, show=True)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "#pp.pprint(story_spans)\n",
        "pp.pprint(story_counts)\n",
        "\n",
        "\n",
        "# Get span of first occurrence\n",
        "first_occurrences = get_first_crime(story_spans)\n",
        "# print(first_occurrences)\n",
        "\n",
        "# Find location of first span\n",
        "first_locations = {}\n",
        "for kw, kw_span in first_occurrences.items():\n",
        "    location = get_span_location(corpus_l, span, kw_span, verbose=False)\n",
        "    first_locations[kw] = location\n",
        "\n",
        "print(story)\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "pp.pprint(first_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqz3wyWlGltJ"
      },
      "source": [
        "### The Boscombe Valley Mystery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af6Ne8K-CPBU"
      },
      "source": [
        "story = 'The Boscombe Valley Mystery'\n",
        "\n",
        "# Get corpus span\n",
        "corpus = get_corpus(story)\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_adventures(corpus_l, n=4)\n",
        "span = spans[0]\n",
        "\n",
        "# Full story\n",
        "story_spans, story_counts = crime_story(CRIME_KEYWORDS, corpus_l, span)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "#pp.pprint(story_spans)\n",
        "pp.pprint(story_counts)\n",
        "\n",
        "\n",
        "# Get span of first occurrence\n",
        "first_occurrences = get_first_crime(story_spans)\n",
        "# print(first_occurrences)\n",
        "\n",
        "# Find location of first span\n",
        "first_locations = {}\n",
        "for kw, kw_span in first_occurrences.items():\n",
        "    location = get_span_location(corpus_l, span, kw_span, verbose=False)\n",
        "    first_locations[kw] = location\n",
        "\n",
        "print(story)\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "pp.pprint(first_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5jJimkLrG5m"
      },
      "source": [
        "#### Utility functions for location of crime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPwEPtO1X562"
      },
      "source": [
        "# Identify paragraph and sentence based on a keyword.\n",
        "# This is used for crime detection analysis.\n",
        "search_pair = ['blood', 'mysterious']\n",
        "gp = 0\n",
        "found = False\n",
        "\n",
        "c = get_chapters(text, span)\n",
        "for k, _c in enumerate(c, start=1):\n",
        "    p = get_paragraphs(text, _c)\n",
        "    for i, _p in enumerate(p, start=1):\n",
        "        gp += 1\n",
        "        s = get_sentences(text, _p)\n",
        "        for j, _s in enumerate(s, start=1):\n",
        "            print(f'C{k}, GP{gp}, S{j}')\n",
        "            t = get_text_from_span(text, _s)\n",
        "            print(f'C{k}, P{i}, S{j} ------------------------\\n', t)\n",
        "            if search_pair[0] in t and search_pair[1] in t:\n",
        "                found = True\n",
        "                break\n",
        "        if found: break\n",
        "    if found: break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_8wGLKhmTRn"
      },
      "source": [
        "# Identify paragraph and sentence based on a keyword.\n",
        "# This is used for crime detection analysis.\n",
        "search_pair = ['murdered', 'man']\n",
        "found = False\n",
        "\n",
        "p = get_paragraphs(text, span)\n",
        "for i, _p in enumerate(p, start=1):\n",
        "    s = get_sentences(text, _p)\n",
        "    for j, _s in enumerate(s, start=1):\n",
        "        t = get_text_from_span(text, _s)\n",
        "        if search_pair[0] in t and search_pair[1] in t:\n",
        "            print(f'P{i}, S{j} ------------------------\\n', t)\n",
        "            found = True\n",
        "            break\n",
        "    if found: break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhtVlQWMAIFc"
      },
      "source": [
        "## Organize data for Jerry's visualization tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkcBMgRUAKzI"
      },
      "source": [
        "# NOTE: Run section 'Crime Detection' first\n",
        "\n",
        "data = []\n",
        "\n",
        "item = collections.OrderedDict()\n",
        "item['title'] = story\n",
        "item['author'] = 'Sir Arthur Conan Doyle'\n",
        "item['queryType'] = 'occurrences'\n",
        "item['question'] = 'Crime'\n",
        "item['numChapters'] = len(story_counts)\n",
        "\n",
        "results = collections.defaultdict(collections.OrderedDict)\n",
        "for chapter, word_map in story_counts.items():\n",
        "    for word, freq in word_map.items():\n",
        "        results[word][str(chapter)] = freq\n",
        "        \n",
        "for word, freq_map in results.items():\n",
        "    if sum(freq_map.values()) < 5:\n",
        "        continue\n",
        "    item['query'] = word\n",
        "    item['results'] = freq_map\n",
        "    data.append(copy.deepcopy(item))\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "pp.pprint(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKNW4eLoAJOc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xJTBNIkkRB-"
      },
      "source": [
        "# Story structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q1TgcxbkXb6"
      },
      "source": [
        "def count_text_structure(text, span=None, structure=None):\n",
        "    if structure is None:\n",
        "        structure = collections.defaultdict(int)\n",
        "\n",
        "    for par_span in get_paragraphs(text, span):\n",
        "        structure['Paragraphs'] += 1\n",
        "        for sent_span in get_sentences(text, par_span):\n",
        "            structure['Sentences'] += 1\n",
        "            for tok_span in get_tokens(text, sent_span):\n",
        "                structure['Words'] += 1\n",
        "    return structure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCvzFwI5qXV5"
      },
      "source": [
        "story = 'The Valley of Fear'\n",
        "corpus = get_corpus(story)\n",
        "text = corpus.lower()\n",
        "structure = collections.defaultdict(int)\n",
        "for part_span in get_parts(text):\n",
        "    structure['Parts'] += 1\n",
        "    for chp_span in get_chapters(text, part_span):\n",
        "        structure['Chapters'] += 1\n",
        "        count_text_structure(text, chp_span, structure)\n",
        "structure['Epilogue'] = 1\n",
        "span = get_text(text)[0]\n",
        "structure['Characters'] = span[1] - span[0]\n",
        "count_text_structure(text, get_epilogue(text)[0], structure)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(structure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G_tuRLPqct3"
      },
      "source": [
        "story = 'A Study in Scarlet'\n",
        "corpus = get_corpus(story)\n",
        "text = corpus.lower()\n",
        "structure = collections.defaultdict(int)\n",
        "for part_span in get_parts(text):\n",
        "    structure['Parts'] += 1\n",
        "    for chp_span in get_chapters(text, part_span):\n",
        "        structure['Chapters'] += 1\n",
        "        count_text_structure(text, chp_span, structure)\n",
        "span = get_text(text)[0]\n",
        "structure['Characters'] = span[1] - span[0]\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(structure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh4204v7q4ov"
      },
      "source": [
        "story = 'The Sign of the Four'\n",
        "corpus = get_corpus(story)\n",
        "text = corpus.lower()\n",
        "structure = collections.defaultdict(int)\n",
        "for chp_span in get_chapters(text):\n",
        "    structure['Chapters'] += 1\n",
        "    count_text_structure(text, chp_span, structure)\n",
        "span = get_text(text)[0]\n",
        "structure['Characters'] = span[1] - span[0]\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(structure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ4K1uGCqua7"
      },
      "source": [
        "story = 'The Hound of the Baskervilles'\n",
        "corpus = get_corpus(story)\n",
        "text = corpus.lower()\n",
        "structure = collections.defaultdict(int)\n",
        "for chp_span in get_chapters(text):\n",
        "    structure['Chapters'] += 1\n",
        "    count_text_structure(text, chp_span, structure)\n",
        "span = get_text(text)[0]\n",
        "structure['Characters'] = span[1] - span[0]\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(structure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E2wmoJoqu1t"
      },
      "source": [
        "story = 'The Boscombe Valley Mystery'\n",
        "corpus = get_corpus(story)\n",
        "text = corpus.lower()\n",
        "structure = collections.defaultdict(int)\n",
        "span = get_adventures(text, n=4)[0]\n",
        "count_text_structure(text, span, structure)\n",
        "structure['Characters'] = span[1] - span[0]\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(structure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeWCIOHwkvRC"
      },
      "source": [
        "# Find character entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUtoC_WUk4Z3"
      },
      "source": [
        "story = 'The Valley of Fear'\n",
        "\n",
        "# Select part(s)\n",
        "# spans = get_parts(get_corpus(story).lower(), n=1)  # n = Part\n",
        "# span = spans[0]  # [i] = Part i+1\n",
        "span = None  # all Parts or if no Parts\n",
        "\n",
        "# Per chapter\n",
        "story_spans, story_counts = ner_story_with_chapters(story, span)\n",
        "plot_ner_counts_story_with_chapters(story, story_counts, show=False)\n",
        "\n",
        "# Full story\n",
        "# story_spans, story_counts = ner_story(story, span)\n",
        "# plot_ner_counts_story(story, story_counts, show=False)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "# pp.pprint(story_spans)\n",
        "pp.pprint(story_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL9Sdxd4V2D2"
      },
      "source": [
        "story = 'A Study in Scarlet'\n",
        "\n",
        "# Select part(s)\n",
        "spans = get_parts(get_corpus(story).lower())  # n = Part\n",
        "span = spans[0]  # [i] = Part i+1\n",
        "\n",
        "# Per chapter\n",
        "story_spans, story_counts = ner_story_with_chapters(story, span)\n",
        "plot_ner_counts_story_with_chapters(story, story_counts, show=True)\n",
        "\n",
        "# Full story\n",
        "# story_spans, story_counts = ner_story(story, span)\n",
        "# plot_ner_counts_story(story, story_counts, show=True)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "# pp.pprint(story_spans)\n",
        "pp.pprint(story_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPUB6HN-oMRj"
      },
      "source": [
        "story = 'The Sign of the Four'\n",
        "\n",
        "# Per chapter\n",
        "story_spans, story_counts = ner_story_with_chapters(story)\n",
        "plot_ner_counts_story_with_chapters(story, story_counts, show=True)\n",
        "\n",
        "# Full story\n",
        "# story_spans, story_counts = ner_story(story)\n",
        "# plot_ner_counts_story(story, story_counts, show=True)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "# pp.pprint(story_spans)\n",
        "pp.pprint(story_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZKxmKulVwmS"
      },
      "source": [
        "story = 'The Hound of the Baskervilles'\n",
        "\n",
        "# Per chapter\n",
        "story_spans, story_counts = ner_story_with_chapters(story)\n",
        "plot_ner_counts_story_with_chapters(story, story_counts, show=True)\n",
        "\n",
        "# Full story\n",
        "# story_spans, story_counts = ner_story(story)\n",
        "# plot_ner_counts_story(story, story_counts, show=True)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "# pp.pprint(story_spans)\n",
        "pp.pprint(story_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaUvweLFGulK"
      },
      "source": [
        "story = 'The Boscombe Valley Mystery'\n",
        "spans = get_adventures(get_corpus(story).lower(), n=4)\n",
        "span = spans[0]\n",
        "\n",
        "# Per paragraph\n",
        "# story_spans, story_counts = ner_story_with_paragraphs(story, span)\n",
        "# plot_ner_counts_story_with_paragraphs(story, story_counts, show=True)\n",
        "\n",
        "# Full story\n",
        "story_spans, story_counts = ner_story(story, span)\n",
        "plot_ner_counts_story(story, story_counts, show=True)\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "# pp.pprint(story_spans)\n",
        "pp.pprint(story_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv_KZyIOw9kk"
      },
      "source": [
        "## Organize data for Jerry's visualization tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h9LiYLcw50r"
      },
      "source": [
        "# NOTE: Run section 'Find character entities' first\n",
        "\n",
        "data = []\n",
        "title = story\n",
        "author = 'Sir Arthur Conan Doyle'\n",
        "query_type = 'occurrences'\n",
        "for character_type, chapter_map in story_counts.items():\n",
        "    if character_type == 'detectives':\n",
        "        question = 'Detective'\n",
        "    elif character_type == 'perpetrators':\n",
        "        question = 'Perpetrator'\n",
        "    elif character_type == 'suspects':\n",
        "        question = 'Other Suspects'\n",
        "    else:\n",
        "        continue\n",
        "    \n",
        "    item = collections.OrderedDict()\n",
        "    item['title'] = title\n",
        "    item['author'] = author\n",
        "    item['queryType'] = query_type\n",
        "    item['question'] = question\n",
        "    item['numChapters'] = len(chapter_map)\n",
        "\n",
        "    characters = CHARACTERS_NAMES[story][character_type]\n",
        "\n",
        "    results = collections.defaultdict(collections.OrderedDict)\n",
        "    aliases = {}\n",
        "    for chapter, character_map in chapter_map.items():\n",
        "        for character, freq in character_map.items():\n",
        "            if character not in results:\n",
        "                for c in characters:\n",
        "                    if c[0] == character:\n",
        "                        aliases[character] = '|'.join([s.replace('_', ' ') for s in c])\n",
        "            results[character][str(chapter)] = freq\n",
        "            \n",
        "    for character, freq_map in results.items():\n",
        "        item['query'] = aliases[character]\n",
        "        item['results'] = freq_map\n",
        "        data.append(copy.deepcopy(item))\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "pp.pprint(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INOc_W1sB7fB"
      },
      "source": [
        "## Identify first-time occurrences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OKynQiwtC9v"
      },
      "source": [
        "### The Valley of Fear, The Sign of the Four, The Hound of the Baskervilles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77nhsqcjCHN9"
      },
      "source": [
        "print(story)\n",
        "\n",
        "# Get span of first occurrence\n",
        "first_occurrences = get_first_occurrences(story_spans)\n",
        "# print(first_occurrences)\n",
        "\n",
        "# Get corpus span\n",
        "corpus_l = get_corpus(story).lower()\n",
        "spans = get_text(corpus_l)\n",
        "span = spans[0]\n",
        "\n",
        "# Find location of first span\n",
        "first_locations = {}\n",
        "for character_type, characters in first_occurrences.items():\n",
        "    first_locations[character_type] = {}\n",
        "    for character in characters:\n",
        "        location = get_span_location_with_chapters(corpus_l, span, first_occurrences[character_type][character], verbose=False)\n",
        "        first_locations[character_type][character] = location\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "pp.pprint(first_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIqDSTkTiZgg"
      },
      "source": [
        "### A Study in Scarlet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pe2X6voifdH"
      },
      "source": [
        "print(story)\n",
        "\n",
        "# Get span of first occurrence\n",
        "first_occurrences = get_first_occurrences(story_spans)\n",
        "# print(first_occurrences)\n",
        "\n",
        "# Get corpus span\n",
        "text = get_corpus(story).lower()\n",
        "spans = get_parts(text)  # n = Part\n",
        "span = spans[0]  # [i] = Part i+1\n",
        "\n",
        "# Find location of first span\n",
        "first_locations = {}\n",
        "for character_type, characters in first_occurrences.items():\n",
        "    first_locations[character_type] = {}\n",
        "    for character in characters:\n",
        "        location = get_span_location_with_chapters(text, span, first_occurrences[character_type][character], verbose=False)\n",
        "        first_locations[character_type][character] = location\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "pp.pprint(first_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPrRVe7wtZlj"
      },
      "source": [
        "### The Boscombe Valley Mystery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JddrNJXupDcI"
      },
      "source": [
        "print(story)\n",
        "\n",
        "# Get span of first occurrence\n",
        "first_occurrences = get_first_occurrences(story_spans)\n",
        "# print(first_occurrences)\n",
        "\n",
        "# Get corpus span\n",
        "text = get_corpus(story).lower()\n",
        "spans = get_adventures(text, n=4)\n",
        "span = spans[0]\n",
        "\n",
        "# Find location of first span\n",
        "first_locations = {}\n",
        "for character_type, characters in first_occurrences.items():\n",
        "    first_locations[character_type] = {}\n",
        "    for character in characters:\n",
        "        location = get_span_location(text, span, first_occurrences[character_type][character], verbose=False)\n",
        "        first_locations[character_type][character] = location\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "pp.pprint(first_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOkZa5g2jlru"
      },
      "source": [
        "## Neighbor words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXfOrm_sjyY-"
      },
      "source": [
        "# NOTE: Need to run \"Find character entities\" as an initial step.\n",
        "\n",
        "print(story_spans)\n",
        "\n",
        "# Merge perpetrators spans across story chapters/sections\n",
        "perpetrators = {}\n",
        "for section, characters in story_spans['perpetrators'].items():\n",
        "    for character, spans in characters.items():\n",
        "        if character not in perpetrators:\n",
        "            perpetrators[character] = []\n",
        "        perpetrators[character].extend(spans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vPCigW2xlYZ"
      },
      "source": [
        "corpus = get_corpus(story)\n",
        "corpus_l = corpus.lower()\n",
        "spans = get_text(corpus_l)\n",
        "#spans = get_chapters(corpus_l)\n",
        "#spans = get_adventures(get_corpus(story).lower(), n=4)\n",
        "\n",
        "tok_spans = get_tokens(corpus_l, spans[0])\n",
        "\n",
        "print(len(tok_spans))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7UK-ti9TT9-"
      },
      "source": [
        "# Method 1: No filtering\n",
        "perpetrator_neighbors = get_neighbor_words_for_character(corpus_l, tok_spans, perpetrators, CHARACTERS_NAMES[story]['perpetrators'])\n",
        "pp.pprint(perpetrator_neighbors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flvPa-wmgG6-"
      },
      "source": [
        "# Method 2: Filter stopwords\n",
        "perpetrator_neighbors_stop = get_neighbor_words_for_character(corpus_l, tok_spans, perpetrators, CHARACTERS_NAMES[story]['perpetrators'], stopwords=STOPWORDS)\n",
        "pp.pprint(perpetrator_neighbors_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiOtszvkjZRz"
      },
      "source": [
        "# Plot top neighboring words\n",
        "top_n = 10\n",
        "\n",
        "for character, freq_modes in perpetrator_neighbors_stop.items():\n",
        "    for mode, freq_all in freq_modes.items():\n",
        "        freq = get_frequent_items(freq_all, top_n)\n",
        "        print(freq)\n",
        "        if freq:\n",
        "            barplot(list(freq.values()), list(freq.keys()))\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwv5aVNkU1DH"
      },
      "source": [
        "## Organize data for Jerry's visualization tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN3GVOFgUsel"
      },
      "source": [
        "data = []\n",
        "\n",
        "item = collections.OrderedDict()\n",
        "item['title'] = story\n",
        "item['author'] = 'Sir Arthur Conan Doyle'\n",
        "item['queryType'] = 'nearby'\n",
        "item['question'] = 'Nearby Words'\n",
        "item['numChapters'] = len(story_counts['perpetrators'])\n",
        "\n",
        "characters = CHARACTERS_NAMES[story]['perpetrators']\n",
        "\n",
        "aliases = {}\n",
        "for character, direction_map in perpetrator_neighbors_stop.items():\n",
        "    for c in characters:\n",
        "        if c[0] == character:\n",
        "            item['query'] = '|'.join([s.replace('_', ' ') for s in c])\n",
        "            break\n",
        "    for direction, freq_map in direction_map.items():\n",
        "        counter = collections.Counter(freq_map)\n",
        "        item[direction] = counter.most_common(3)\n",
        "    data.append(copy.deepcopy(item))\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "pp.pprint(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kBPu0fK-6yr"
      },
      "source": [
        "# EOF"
      ]
    }
  ]
}